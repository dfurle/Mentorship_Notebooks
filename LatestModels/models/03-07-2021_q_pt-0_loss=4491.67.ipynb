{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Te6rdZgT5yAW"
   },
   "source": [
    "# Copy of Previous RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdsGeayZ523e"
   },
   "source": [
    "### Reminders:\n",
    "\n",
    "Read up on some of these:\n",
    "- https://machinelearningmastery.com/stateful-stateless-lstm-time-series-forecasting-python/\n",
    "- https://fairyonice.github.io/Stateful-LSTM-model-training-in-Keras.html \n",
    "- https://github.com/keras-team/keras/issues/5714\n",
    "- https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/\n",
    "\n",
    "###Shuffle Data!!!!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ro1oh9-25xM9",
    "outputId": "e2638721-dec3-4b6c-9db0-df28d7fce705"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7yOaxkJ58hh"
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5Lupm0J6Hre"
   },
   "source": [
    "## Do Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gE8NHB9v6IDc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import pandas\n",
    "from matplotlib import pyplot as plt\n",
    "from math import isnan\n",
    "from scipy.stats import norm\n",
    "\n",
    "pandas.set_option('display.max_columns', None)\n",
    "np.set_printoptions(suppress=True, precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yhg_bSj36CSt"
   },
   "source": [
    "## Obtain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1LMf25vB5_vs"
   },
   "outputs": [],
   "source": [
    "# read data from drive\n",
    "# csv_train = pandas.read_csv(\"drive/MyDrive/first_10k.csv\")\n",
    "csv_train = pandas.read_csv(\"first_10k.csv\")\n",
    "# csv_train = pandas.read_csv(\"drive/MyDrive/FDC_tracks.csv\")\n",
    "unparsed_train = np.array(csv_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7L54wTFh-NGU"
   },
   "source": [
    "## Ragged Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KoN7P86yCif6",
    "outputId": "b9ad9880-a607-4ac8-dd65-3e3a802f4a7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47.588573718740314, 47.716949659038654, 47.92074125234375, 0.0433333333333333, 96.0, 24.0, 343.50521373311966, 274.8641357421875, 3.24600255369344e-07, 137.76808719278492]\n",
      "[-47.69482374702719, -47.64816097490714, -47.78196688638672, 0.0100725857504721, 1.0, 1.0, 176.8656847042481, -77.76896667480467, 7.068405941829982e-10, 0.2999999999999999]\n",
      "[53.93938010927168, 182.16740477086037, 20.384216393930128, 80.4857546970404]\n",
      "[-59.97950523099952, -113.91363367061092, -21.351611455494343, -101.41199494730319]\n"
     ]
    }
   ],
   "source": [
    "_max = [-1000 for i in range(10)]\n",
    "_min = [1000 for i in range(10)]\n",
    "for index,event in enumerate(unparsed_train):\n",
    "  lower = 67\n",
    "  for upper in range(lower+14, event.shape[0]+1, 14):\n",
    "    d = event[lower:upper]\n",
    "    d = np.append(d[:2],d[6:])\n",
    "    for a in range(len(d)):\n",
    "      if d[a] > _max[a]:\n",
    "        _max[a] = d[a]\n",
    "      if d[a] < _min[a]:\n",
    "        _min[a] = d[a]\n",
    "    lower = upper\n",
    "print(_max)\n",
    "print(_min)\n",
    "\n",
    "_TOF_max = [-1000 for i in range(4)]\n",
    "_TOF_min = [1000 for i in range(4)]\n",
    "for index,event in enumerate(unparsed_train):\n",
    "  TOF = event[59:67]\n",
    "  # print(TOF)\n",
    "  for a in range(4):\n",
    "    if TOF[a*2] > _TOF_max[a]:\n",
    "      _TOF_max[a] = TOF[a*2]\n",
    "    if TOF[a*2] < _TOF_min[a]:\n",
    "      _TOF_min[a] = TOF[a*2]\n",
    "print(_TOF_max)\n",
    "print(_TOF_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOfDY5F8yJyl"
   },
   "source": [
    "After trying out many different possibilities, I have found making a RaggedTensor is the way to make variable timesteps.\n",
    "\n",
    "Once you create a RaggedTensor ONLY FOR X DATA, you need to also add:\n",
    "\n",
    "ragged=True\n",
    "\n",
    "to the keras.Inputs() function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BfACUWvW-NGd"
   },
   "outputs": [],
   "source": [
    "def ragged_parser(unparsed):\n",
    "  global _min, _max\n",
    "  x_final = []\n",
    "  y_final = []\n",
    "  invCov_final = []\n",
    "  cov_final = []\n",
    "  for event in unparsed:\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    nEvent = event[0]     # all the data split into neat little arrays...\n",
    "    state = event[1:6]\n",
    "    coVar = event[6:31]\n",
    "    invCoVar = event[31:56]\n",
    "    goodnessOfFit = event[56:59]\n",
    "    TOF = event[59:67]\n",
    "\n",
    "    if goodnessOfFit[2] > 0.1:   # Cutting if rms is too high\n",
    "      continue\n",
    "    hits = []\n",
    "    lower = 67\n",
    "    for upper in range(lower+14, event.shape[0]+1, 14): # to flip just go from end to 67 by -14 steps?\n",
    "\n",
    "      # hasNAN = False\n",
    "      # for val in event[lower:upper]:\n",
    "      #   if isnan(val):\n",
    "      #     hasNAN = True\n",
    "      # if not hasNAN:\n",
    "\n",
    "      if not isnan(event[lower]):            # Check if we are done with hits, because data is cut short, the rest will be nan\n",
    "        hit_data = event[lower:upper]                      # retrieving the hit\n",
    "        hit_data = np.append(hit_data[:2],hit_data[6:])    # cutting out the sin and cos data\n",
    "        for z in range(len(hit_data)):\n",
    "          hit_data[z] = (hit_data[z] - _min[z]) / (_max[z] - _min[z])    # we need to normalize the data; this can be moved to a lambda layer in the network if needed.\n",
    "        for i_TOF in range(4):\n",
    "          TOF[i_TOF*2] = (TOF[i_TOF*2] - _TOF_min[i_TOF]) / (_TOF_max[i_TOF] - _TOF_min[i_TOF])\n",
    "        hit_data = np.append(hit_data,TOF)\n",
    "        hits.append(np.ndarray.tolist(hit_data))       # we want it as a list to convert to RaggedTensor later; last time I checked it didnt work with array.\n",
    "      lower = upper\n",
    "    for i in range(len(hits)):   # this could be simplified to just: \"x = hits\" if im not mistaken...\n",
    "      x.append(hits[i])          # however we might need to add y.append(hits[i+1]) for later testing so leaving it like this for now...\n",
    "    y = np.ndarray.tolist(state)   # technically not needed, can be removed later... at first I thought i need to pass RaggedTensor labels, but that is not the case.\n",
    "    x_final.append(x)          # want x_final to be shape (event, hit, 10) as a list\n",
    "#     y_final.append(y)          # want y_final to be shape (event, 5)       as a np.array\n",
    "    y_final.append(y[0])          # want y_final to be shape (event, 5)       as a np.array\n",
    "    invCov_final.append(invCoVar[:])  # want other_f to be shape (event, 25)      as a np.array\n",
    "    cov_final.append(coVar[:])\n",
    "  x_final = tf.ragged.constant(x_final)   # convert list to RaggedTensor because timesteps (number of hits) are variable between events\n",
    "  y_final = np.array(y_final)\n",
    "  invCov_final = np.array(invCov_final)\n",
    "  cov_final = np.array(cov_final)\n",
    "  return [x_final, invCov_final, cov_final, y_final], y_final   # with the custom loss the x_train (input) needs to be a list of [inputs, inverseCovariance, labels]\n",
    "  # return [x_final, invCov_final], y_final   # with the custom loss the x_train (input) needs to be a list of [inputs, inverseCovariance, labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J7H0PK6Q-NGg",
    "outputId": "6fcb6671-f32c-402a-c1a3-8489231a8d75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--==Types==--\n",
      "--x_train:--\n",
      "\n",
      "  -> input_data: x_train[0]\n",
      "  -> type expected: RaggedTensor\n",
      " <class 'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor'>\n",
      "\n",
      "  -> invCov: x_train[1]\n",
      "  -> type expected: np.array\n",
      " <class 'numpy.ndarray'>\n",
      "\n",
      "  -> y_train: x_train[2]\n",
      "  -> type expected: np.array\n",
      " <class 'numpy.ndarray'>\n",
      "\n",
      "--y_train:--\n",
      "\n",
      "  -> y_train: y_train\n",
      "  -> type expected: np.array\n",
      " <class 'numpy.ndarray'>\n",
      "\n",
      "\n",
      "--==Shapes==--\n",
      "--x_train:--  \n",
      "num of events: 7729\n",
      "\n",
      "  RaggedTensor | Input:  shape = (7729, 18, 18)\n",
      "  np.array     | InvCov: shape = (7729, 25)\n",
      "  np.array     | Labels: shape = (7729, 25)\n",
      "\n",
      "--x_train:--\n",
      "  np.array     | Labels: shape = (7729,)\n",
      "x_train : <tf.RaggedTensor [[0.7692640423774719, 0.765606164932251, 0.22407019138336182, 0.2642019987106323, 0.49473685026168823, 0.9130434989929199, 0.9742012023925781, 0.8189674615859985, 0.006027945317327976, 0.006027945317327976, 0.5265106558799744, 0.0, 0.4158459007740021, 1.0, 0.511589527130127, 0.0, 0.5575219988822937, 0.0], [0.6926482319831848, 0.5690751671791077, 0.3645309507846832, 0.05952613055706024, 0.2631579041481018, 0.8695651888847351, 0.9611853361129761, 0.28649309277534485, 0.033259935677051544, 0.033259935677051544, 0.5311324596405029, 0.0, 0.3861425220966339, 1.0, 0.5238472819328308, 0.0, 0.5605869889259338, 0.0], [0.23875504732131958, 0.24075621366500854, 0.7672280669212341, 0.10457572340965271, 0.5052631497383118, 0.782608687877655, 0.9352953433990479, 0.504021406173706, 0.018304066732525826, 0.018304066732525826, 0.5311729907989502, 0.0, 0.38604220747947693, 1.0, 0.5241410136222839, 0.0, 0.5606038570404053, 0.0], [0.33787238597869873, 0.4484703540802002, 0.6097946166992188, 0.07363839447498322, 0.7157894968986511, 0.739130437374115, 0.7676599621772766, 0.33562812209129333, 0.02666318602859974, 0.02666318602859974, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241480469703674, 0.0, 0.5606039762496948, 0.0], [0.5710769891738892, 0.6744455695152283, 0.37305063009262085, 0.288280189037323, 0.7052631378173828, 0.695652186870575, 0.7548211812973022, 0.7309820652008057, 0.005347346421331167, 0.005347346421331167, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.733315110206604, 0.7239073514938354, 0.263613760471344, 0.041055768728256226, 0.4842105209827423, 0.6521739363670349, 0.7418311238288879, 0.5430606007575989, 0.048400089144706726, 0.048400089144706726, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.6543219089508057, 0.5472460389137268, 0.39544129371643066, 0.12399385124444962, 0.2947368323802948, 0.6086956262588501, 0.728915810585022, 0.5731130242347717, 0.015151274390518665, 0.015151274390518665, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.42824169993400574, 0.3272317349910736, 0.6255077123641968, 0.04272006079554558, 0.3052631616592407, 0.5652173757553101, 0.7160916924476624, 0.30231329798698425, 0.046525269746780396, 0.046525269746780396, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.2750035524368286, 0.28259047865867615, 0.7278001308441162, 0.024007415398955345, 0.5157894492149353, 0.52173912525177, 0.703081488609314, 0.3300199508666992, 0.08132698386907578, 0.08132698386907578, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.3927527666091919, 0.47597992420196533, 0.5672175288200378, 0.08297774195671082, 0.6631578803062439, 0.47826087474823, 0.4162442684173584, 0.5024588704109192, 0.023499751463532448, 0.023499751463532448, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.567786455154419, 0.6398308277130127, 0.392142653465271, 0.030097603797912598, 0.6421052813529968, 0.43478259444236755, 0.40303486585617065, 0.2945617437362671, 0.06557145714759827, 0.06557145714759827, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.6758876442909241, 0.6613656878471375, 0.3252999782562256, 0.05598178505897522, 0.4736842215061188, 0.3913043439388275, 0.39034584164619446, 0.45713284611701965, 0.035419683903455734, 0.035419683903455734, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.6067799925804138, 0.5218054056167603, 0.433348685503006, 0.06648389250040054, 0.3368421196937561, 0.3478260934352875, 0.3770085275173187, 0.44806209206581116, 0.029668668285012245, 0.029668668285012245, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.33219149708747864, 0.3447727859020233, 0.6660985350608826, 0.02286168746650219, 0.5263158082962036, 0.260869562625885, 0.35123491287231445, 0.42745885252952576, 0.085147425532341, 0.085147425532341, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.436598539352417, 0.4929949641227722, 0.5362541675567627, 0.03817495331168175, 0.6105263233184814, 0.21739129722118378, 0.0648939236998558, 0.2867984175682068, 0.052010804414749146, 0.052010804414749146, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.6153600811958313, 0.6007120013237, 0.38774821162223816, 0.04457264766097069, 0.4736842215061188, 0.1304347813129425, 0.038748566061258316, 0.4948866367340088, 0.04459531232714653, 0.04459531232714653, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.45056793093681335, 0.40904945135116577, 0.5723205208778381, 0.04106782376766205, 0.42105263471603394, 0.043478261679410934, 0.012867040000855923, 0.33683282136917114, 0.048385992646217346, 0.048385992646217346, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0], [0.3925012946128845, 0.4053120017051697, 0.6041624546051025, 0.032487500458955765, 0.5263158082962036, 0.0, 0.00017478904919698834, 0.65003901720047, 0.060900986194610596, 0.060900986194610596, 0.5311733484268188, 0.0, 0.3860418498516083, 1.0, 0.5241482257843018, 0.0, 0.5606039762496948, 0.0]]>\n",
      "y_train : -4.80515106805654\n"
     ]
    }
   ],
   "source": [
    "# split = 450000\n",
    "# split = 3\n",
    "# split = int(unparsed_train.shape[0]*0.90)\n",
    "x_train, y_train = ragged_parser(unparsed_train[0:8300]) # shuffle data before taking\n",
    "x_test, y_test = ragged_parser(unparsed_train[8300:])\n",
    "# x_train, y_train = ragged_parser(unparsed_train[:split])\n",
    "# x_test , y_test  = ragged_parser(unparsed_train[split:split+100])\n",
    "\n",
    "print(\"--==Types==--\")\n",
    "print(\"--x_train:--\")\n",
    "print(\"\\n  -> input_data: x_train[0]\\n  -> type expected: RaggedTensor\\n \"+str(type(x_train[0])))\n",
    "print(\"\\n  -> invCov: x_train[1]\\n  -> type expected: np.array\\n \"+str(type(x_train[1])))\n",
    "print(\"\\n  -> y_train: x_train[2]\\n  -> type expected: np.array\\n \"+str(type(x_train[2])))\n",
    "print(\"\\n--y_train:--\")\n",
    "print(\"\\n  -> y_train: y_train\\n  -> type expected: np.array\\n \"+str(type(y_train)))\n",
    "\n",
    "print(\"\\n\\n--==Shapes==--\")\n",
    "print(\"--x_train:--  \\nnum of events: \" + str(x_train[0].shape[0]))\n",
    "print(\"\\n  RaggedTensor | Input:  shape = \" + \"(\" + str(x_train[0].shape[0]) + \", \"+ str(x_train[0][0].shape[0]) + \", \"+ str(x_train[0][0][0].shape[0]) + \")\")\n",
    "print(\"  np.array     | InvCov: shape = \" + str(x_train[1].shape))\n",
    "print(\"  np.array     | Labels: shape = \" + str(x_train[2].shape))\n",
    "print(\"\\n--x_train:--\")\n",
    "print(\"  np.array     | Labels: shape = \" + str(y_train.shape))\n",
    "\n",
    "print(\"x_train : \" + str(x_train[0][0]))\n",
    "print(\"y_train : \" + str(y_train[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMvnihIl6ail"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdreWH016c08"
   },
   "source": [
    "## Defining Models\n",
    "\n",
    "- model\n",
    "  - Very basic testing RNN model\n",
    "  - Output every timestep\n",
    "\n",
    "- model_timeless\n",
    "  - Very basic testing RNN model\n",
    "  - Output only at the end\n",
    "\n",
    "- RNNTime\n",
    "  - Advanced\n",
    "  - Time distributed, output every timestep\n",
    "\n",
    "- RNNTimeless\n",
    "  - Advanced\n",
    "  - Only output at final layer\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "e8qCwtNw6bFk"
   },
   "outputs": [],
   "source": [
    "def model(x):\n",
    "  x = keras.layers.LSTM(64,activation=\"tanh\", name='input_lstm1', return_sequences=True)(x)\n",
    "  x = keras.layers.TimeDistributed(keras.layers.Dense(32, activation='relu'), name=\"TD1-Dense\")(x)\n",
    "  x = keras.layers.TimeDistributed(keras.layers.Dense(14, activation='linear'), name=\"output-Dense\")(x)\n",
    "  return x\n",
    "def model_timeless(x):\n",
    "  x = keras.layers.LSTM(64,activation=\"tanh\", name='input_lstm1', return_sequences=False)(x)\n",
    "  x = keras.layers.Dense(32, activation='relu', name=\"Dense1\")(x)\n",
    "  x = keras.layers.Dense(5, activation='relu', name=\"output-Dense\")(x)\n",
    "  return x\n",
    "\n",
    "def RNNTime(x):\n",
    "  x = keras.layers.LSTM(128,activation=\"tanh\", name='input_lstm1', stateful=False, return_sequences=True)(x)\n",
    "  x = keras.layers.LSTM(64,activation=\"tanh\", name='lstm2', stateful=False, return_sequences=True)(x)\n",
    "  x = keras.layers.LSTM(32,activation=\"tanh\", name='lstm3', stateful=False, return_sequences=True)(x)\n",
    "  x = keras.layers.TimeDistributed(keras.layers.Dense(32, activation='relu'), name=\"TD1-Dense\")(x)\n",
    "  x = keras.layers.TimeDistributed(keras.layers.Dense(5, activation='linear'), name=\"output-Dense\")(x)\n",
    "  return x\n",
    "\n",
    "def RNNTimeless(x):\n",
    "  x = keras.layers.LSTM(128,activation=\"tanh\", name='input_lstm1', stateful=False, return_sequences=True)(x)\n",
    "  x = keras.layers.LSTM(64,activation=\"tanh\", name='lstm2', stateful=False, return_sequences=True)(x)\n",
    "  x = keras.layers.LSTM(64,activation=\"tanh\", name='lstm3', stateful=False, return_sequences=False)(x)\n",
    "  x = keras.layers.Dense(32, activation='relu', name=\"Dense1\")(x)\n",
    "  x = keras.layers.Dense(1, activation='linear', name=\"output-Dense\")(x)\n",
    "  # x = keras.layers.Dense(5, activation='linear', name=\"output-Dense\")(x)\n",
    "  # x = keras.layers.lambda(# normalize)\n",
    "  return x\n",
    "\n",
    "\n",
    "# def RNNTimeless(x):\n",
    "#   x = keras.layers.LSTM(128,activation=\"tanh\", name='input_lstm1', stateful=False, return_sequences=True)(x)\n",
    "#   x = keras.layers.LSTM(64,activation=\"tanh\", name='lstm2', stateful=False, return_sequences=True)(x)\n",
    "#   x = keras.layers.LSTM(64,activation=\"tanh\", name='lstm3', stateful=False, return_sequences=False)(x)\n",
    "#   x = keras.layers.Dense(32, activation='relu', name=\"Dense1\")(x)\n",
    "#   x = keras.layers.Dense(5, activation='linear', name=\"output-Dense\")(x)\n",
    "#   # x = keras.layers.lambda(# normalize)\n",
    "#   return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gtf37tFTB5HU"
   },
   "source": [
    "## Custom Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOZIj6R8u0VG"
   },
   "source": [
    "### V1 Originial, unedited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "v8iA9da3HLBs"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "#--------------------------------------------\n",
    "# Define custom loss function \n",
    "def customLoss(y_true, y_pred, invcov):\n",
    "  # print(type(y_true))    #<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
    "\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "  print('y_pred shape: ' + str(y_pred.shape) )  # y_pred shape is (batch, 5)\n",
    "  print('y_true shape: ' + str(y_true.shape) )  # y_true shape is (batch, 5)\n",
    "  print('invcov shape: ' + str(invcov.shape) )  # invcov shape is (batch, 25)\n",
    "  \n",
    "  y_pred = K.reshape(y_pred, (batch_size, 5,1)) # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = K.reshape(y_true, (batch_size, 5,1)) # y_state shape is now (batch, 5,1)\n",
    "  invcov = K.reshape(invcov, (batch_size, 5,5)) # invcov  shape is now (batch, 5,5)\n",
    "  \n",
    "  # n.b. we must use tf.transpose here an not K.transpose since the latter does not allow perm argument\n",
    "  invcov = tf.transpose(invcov, perm=[0,2,1])     # invcov shape is now (batch, 5,5)\n",
    "  \n",
    "  # Difference between prediction and true state vectors\n",
    "  y_diff = y_pred - y_true\n",
    "\n",
    "  # n.b. use \"batch_dot\" and not \"dot\"!\n",
    "  y_dot = K.batch_dot(invcov, y_diff)           # y_dot shape is (batch,5,1)\n",
    "  y_dot = K.reshape(y_dot, (batch_size, 1, 5))  # y_dot shape is now (batch,1,5)\n",
    "  y_loss = K.batch_dot(y_dot, y_diff)           # y_loss shape is (batch,1,1)\n",
    "  y_loss = K.reshape(y_loss, (batch_size,))     # y_loss shape is now (batch)\n",
    "  return y_loss\n",
    "\n",
    "#--------------------------------------------\n",
    "# Test loss function\n",
    "# x_test = x_train[2][0]\n",
    "# y_test = model.predict([x_train[0][0:1],x_train[1][0:1],x_train[2][0:1]])\n",
    "# y_test = np.squeeze(y_test)\n",
    "# inconv_test = x_train[1][0]\n",
    "\n",
    "# loss = K.eval(customLoss(K.variable([x_test,x_test,x_test]), K.variable([y_test,y_test,y_test]), K.variable([inconv_test,inconv_test,inconv_test])))\n",
    "# print('loss shape: '    + str(loss.shape)    )\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXxfMKTqTvBC"
   },
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZX3nAlhgTu4h"
   },
   "outputs": [],
   "source": [
    "def customMetric(y_true, y_pred, cov, id=0):\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "\n",
    "  y_pred = K.reshape(y_pred, (batch_size, 5,1)) # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = K.reshape(y_true, (batch_size, 5,1)) # y_state shape is now (batch, 5,1)\n",
    "  cov = K.reshape(cov, (batch_size, 5,5)) # cov  shape is now (batch, 5,5)\n",
    "  # cov = tf.transpose(cov, perm=[0,2,1])     # cov shape is now (batch, 5,5)\n",
    "  y_diff = y_pred[:,id] - y_true[:,id]\n",
    "  # y_diff = K.reshape(y_diff, (batch_size,1))\n",
    "  cov = K.reshape(cov[:,id,id], (batch_size,1))\n",
    "  # print(\"diff:\\n\",y_diff)\n",
    "  print(\"cov:\\n\",cov)\n",
    "  # return (y_diff*y_diff)/(cov[:,id,id])\n",
    "  return tf.math.square(y_diff)/(cov)\n",
    "\n",
    "# ccov = x_train[2][0:6]\n",
    "# ccov = np.reshape(ccov, (6,5,5))\n",
    "\n",
    "# print(ccov)\n",
    "\n",
    "# metric = K.eval(customMetric(y_train[0:6],y_train[1:7],x_train[2][0:6],0))\n",
    "# print(\"metric: \\n\",metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customMetric(y_true, y_pred, cov, id=0):\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "\n",
    "  y_pred = K.reshape(y_pred, (batch_size, 1)) # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = K.reshape(y_true, (batch_size, 1)) # y_state shape is now (batch, 5,1)\n",
    "  cov = K.reshape(cov, (batch_size, 5,5)) # cov  shape is now (batch, 5,5)\n",
    "  # cov = tf.transpose(cov, perm=[0,2,1])     # cov shape is now (batch, 5,5)\n",
    "  y_diff = y_pred - y_true\n",
    "  # y_diff = K.reshape(y_diff, (batch_size,1))\n",
    "  cov = K.reshape(cov[:,id,id], (batch_size,1))\n",
    "  # print(\"diff:\\n\",y_diff)\n",
    "  print(\"cov:\\n\",cov)\n",
    "  print(\"diff:\\n\",y_diff)\n",
    "  # return (y_diff*y_diff)/(cov[:,id,id])\n",
    "  return tf.math.square(y_diff)/(cov)\n",
    "\n",
    "# ccov = x_train[2][0:6]\n",
    "# ccov = np.reshape(ccov, (6,5,5))\n",
    "\n",
    "# print(ccov)\n",
    "\n",
    "# metric = K.eval(customMetric(y_train[0:6,0],y_train[1:7,0],x_train[2][0:6],3))\n",
    "# print(\"metric: \\n\",metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qw41A73h-zGB"
   },
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SZ7d8PMx652m",
    "outputId": "aab903f4-7785-4519-ef41-4067f641d5e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cov:\n",
      " Tensor(\"Reshape_3:0\", shape=(None, 1), dtype=float32)\n",
      "diff:\n",
      " Tensor(\"Sub:0\", shape=(None, 1), dtype=float32)\n",
      "Model: \"RNNModel\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 18)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_lstm1 (LSTM)              (None, None, 128)    75264       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm2 (LSTM)                    (None, None, 64)     49408       input_lstm1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm3 (LSTM)                    (None, 64)           33024       lstm2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Dense1 (Dense)                  (None, 32)           2080        lstm3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "output-Dense (Dense)            (None, 1)            33          Dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           output-Dense[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 1)]          0           output-Dense[0][0]               \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 1)]          0           input_2[0][0]                    \n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 5, 5)]       0           input_4[0][0]                    \n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub (TensorFlowOpLa [(None, 1)]          0           tf_op_layer_Reshape[0][0]        \n",
      "                                                                 tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(None,)]            0           tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3/shape (Te [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Square (TensorFlowO [(None, 1)]          0           tf_op_layer_Sub[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 1)]          0           tf_op_layer_strided_slice_1[0][0]\n",
      "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv (TensorFlow [(None, 1)]          0           tf_op_layer_Square[0][0]         \n",
      "                                                                 tf_op_layer_Reshape_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_metric (AddMetric)          (None, 1)            0           tf_op_layer_RealDiv[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 159,809\n",
      "Trainable params: 159,809\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --==Not in use?==--\n",
    "# lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate=1e-3,\n",
    "#     decay_steps=10000,\n",
    "#     decay_rate=0.8)\n",
    "from keras.layers import Dense\n",
    "\n",
    "# nInput = 10\n",
    "nInput = 18\n",
    "\n",
    "# --==Set seed to get identical results==-- begin\n",
    "# from tensorflow.random import set_seed\n",
    "# np.random.seed(1)\n",
    "# set_seed(2)\n",
    "# --==Set seed to get identical results==-- end\n",
    "\n",
    "#--==Set Weights==--\n",
    "# loss_weights = [1/(sd**2)]\n",
    "# loss_weights = np.array(loss_weights)/sum(loss_weights)\n",
    "# model.compile(optimizer=optimizer, loss=\"mse\", loss_weights=loss_weights, metrics=[\"mae\"])\n",
    "\n",
    "inputs = keras.Input((None,nInput))\n",
    "# input_true = keras.Input((5,))\n",
    "input_true = keras.Input((1,))\n",
    "input_incov = keras.Input((25,))\n",
    "input_cov_f = keras.Input((25,))\n",
    "all_inputs = [inputs, input_incov, input_cov_f, input_true]\n",
    "# all_inputs = [inputs, input_incov, input_true]\n",
    "\n",
    "# --==Choose model==--\n",
    "# x = model(inputs)\n",
    "# x = model_timeless(inputs)\n",
    "# x = RNNTime(inputs)\n",
    "x = RNNTimeless(inputs)\n",
    "# x = RNNTimeStateful(inputs)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "# outs = {\n",
    "#     \"q_pt\":Dense(1, name=\"q_pt\")(x),\n",
    "#     \"phi\":Dense(1, name=\"phi\")(x),\n",
    "#     \"tanl\":Dense(1, name=\"tanl\")(x),\n",
    "#     \"D\":Dense(1, name=\"D\")(x),\n",
    "#     \"z\":Dense(1, name=\"z\")(x)\n",
    "# }\n",
    "\n",
    "# y_dict = {\n",
    "#     \"q_pt\":y_train[:,0],\n",
    "#     \"phi\":y_train[:,1],\n",
    "#     \"tanl\":y_train[:,2],\n",
    "#     \"D\":y_train[:,3],\n",
    "#     \"z\":y_train[:,4]\n",
    "# }\n",
    "\n",
    "# model = keras.Model(inputs=all_inputs, outputs=outs, name=\"RNNModel\")\n",
    "\n",
    "model = keras.Model(inputs=all_inputs, outputs=x, name=\"RNNModel\")\n",
    "\n",
    "# model.add_metric(customMetric(input_true, x, input_cov_f, 0),name=\"q_pt\")\n",
    "# model.add_metric(customMetric(input_true, x, input_cov_f, 1),name=\"phi\")\n",
    "# model.add_metric(customMetric(input_true, x, input_cov_f, 2),name=\"tanl\")\n",
    "# model.add_metric(customMetric(input_true, x, input_cov_f, 3),name=\"D\")\n",
    "# model.add_metric(customMetric(input_true, x, input_cov_f, 4),name=\"z\")\n",
    "\n",
    "# model.add_loss(customLoss(input_true, x, input_incov))\n",
    "# model.compile(loss=None, optimizer=optimizer, metrics=[\"mae\"])\n",
    "\n",
    "# model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "model.add_metric(customMetric(input_true, x, input_cov_f, 0),name=\"customMetric\")\n",
    "# try as loss\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"mae\"])\n",
    "# model.compile(loss=\"\", optimizer=optimizer, metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkjx1-6Gr76F"
   },
   "source": [
    "### Custom Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "t2pxxUfH7sjY"
   },
   "outputs": [],
   "source": [
    "def concat_hist(H1,H2):\n",
    "  H = {}\n",
    "  for i in H1.keys():\n",
    "    H[i] = list(np.append(np.array(H1[i]),np.array(H2[i])))\n",
    "  return H\n",
    "H = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sjLh8Eh07HRW",
    "outputId": "e8312305-0acd-4240-c575-5b48161d1655"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "31/31 [==============================] - 2s 66ms/step - loss: 4644.8613 - mae: 22.5942 - customMetric: 87126.3672\n",
      "Epoch 2/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4646.2261 - mae: 22.3870 - customMetric: 78727.0234\n",
      "Epoch 3/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4727.8003 - mae: 23.0297 - customMetric: 85674.2656\n",
      "Epoch 4/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4831.0591 - mae: 23.3508 - customMetric: 85214.5234\n",
      "Epoch 5/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4626.7197 - mae: 22.7070 - customMetric: 85055.5938\n",
      "Epoch 6/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4834.1050 - mae: 23.0096 - customMetric: 89490.1172\n",
      "Epoch 7/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4895.9634 - mae: 23.3673 - customMetric: 88428.5234\n",
      "Epoch 8/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4884.8618 - mae: 23.6429 - customMetric: 77972.6953\n",
      "Epoch 9/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4756.8159 - mae: 23.1867 - customMetric: 80504.6250\n",
      "Epoch 10/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4668.4712 - mae: 22.6859 - customMetric: 79727.9922\n",
      "Epoch 11/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4876.8765 - mae: 23.1591 - customMetric: 82837.3438\n",
      "Epoch 12/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4988.8198 - mae: 23.4078 - customMetric: 59773.3125\n",
      "Epoch 13/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4778.5547 - mae: 23.2219 - customMetric: 70841.3672\n",
      "Epoch 14/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4677.9209 - mae: 22.9986 - customMetric: 85613.7891\n",
      "Epoch 15/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4744.3706 - mae: 23.0069 - customMetric: 93136.7031\n",
      "Epoch 16/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4697.3926 - mae: 23.2977 - customMetric: 100317.6875\n",
      "Epoch 17/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4633.7661 - mae: 23.0352 - customMetric: 102140.4844\n",
      "Epoch 18/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4654.8232 - mae: 22.6451 - customMetric: 83713.6406\n",
      "Epoch 19/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4627.3774 - mae: 22.9845 - customMetric: 85148.8438\n",
      "Epoch 20/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4616.5786 - mae: 22.7769 - customMetric: 75945.6250\n",
      "Epoch 21/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4525.0552 - mae: 22.6647 - customMetric: 92054.7500\n",
      "Epoch 22/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4520.7354 - mae: 22.3083 - customMetric: 74440.5234\n",
      "Epoch 23/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4546.4531 - mae: 22.4129 - customMetric: 77019.3984\n",
      "Epoch 24/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4567.7998 - mae: 22.5167 - customMetric: 80872.4922\n",
      "Epoch 25/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4516.2866 - mae: 22.4955 - customMetric: 75539.8750\n",
      "Epoch 26/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4535.8867 - mae: 22.4075 - customMetric: 75547.0547\n",
      "Epoch 27/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 5474.5454 - mae: 25.1178 - customMetric: 121636.6016\n",
      "Epoch 28/300\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 5443.3838 - mae: 24.9515 - customMetric: 100072.0859\n",
      "Epoch 29/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 5375.1050 - mae: 24.3390 - customMetric: 93445.2969\n",
      "Epoch 30/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4990.4438 - mae: 23.8918 - customMetric: 101756.4141\n",
      "Epoch 31/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4858.5645 - mae: 23.2784 - customMetric: 94002.3125\n",
      "Epoch 32/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4955.2876 - mae: 23.6985 - customMetric: 103405.2812\n",
      "Epoch 33/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4728.5435 - mae: 22.8658 - customMetric: 82329.1641\n",
      "Epoch 34/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 5969.8701 - mae: 26.3112 - customMetric: 75793.3828\n",
      "Epoch 35/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 5139.1880 - mae: 24.3891 - customMetric: 122631.9141\n",
      "Epoch 36/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4941.4106 - mae: 23.6788 - customMetric: 92145.2500\n",
      "Epoch 37/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4582.9434 - mae: 22.8076 - customMetric: 78734.9609\n",
      "Epoch 38/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4517.5981 - mae: 22.5529 - customMetric: 76522.0781\n",
      "Epoch 39/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4583.3882 - mae: 22.6242 - customMetric: 70070.9531\n",
      "Epoch 40/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4513.4507 - mae: 22.3129 - customMetric: 78475.8047\n",
      "Epoch 41/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4478.0610 - mae: 22.4994 - customMetric: 68133.5156\n",
      "Epoch 42/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4456.1973 - mae: 22.2680 - customMetric: 73289.3047\n",
      "Epoch 43/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4508.0200 - mae: 22.5240 - customMetric: 70319.0156\n",
      "Epoch 44/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4493.2188 - mae: 22.3073 - customMetric: 55957.5117\n",
      "Epoch 45/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4426.6387 - mae: 22.3579 - customMetric: 83259.7422\n",
      "Epoch 46/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4428.5391 - mae: 22.1282 - customMetric: 78204.1797\n",
      "Epoch 47/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4513.4629 - mae: 22.5446 - customMetric: 73047.8828\n",
      "Epoch 48/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4766.7866 - mae: 23.1287 - customMetric: 78030.6875\n",
      "Epoch 49/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4613.0020 - mae: 22.9021 - customMetric: 72044.7422\n",
      "Epoch 50/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4837.0249 - mae: 23.0922 - customMetric: 92578.1953\n",
      "Epoch 51/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4601.1812 - mae: 22.9497 - customMetric: 85296.9219\n",
      "Epoch 52/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4468.3833 - mae: 22.4433 - customMetric: 77736.1484\n",
      "Epoch 53/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4365.9126 - mae: 21.9628 - customMetric: 69273.1953\n",
      "Epoch 54/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4375.8037 - mae: 22.2182 - customMetric: 82985.8672\n",
      "Epoch 55/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4474.0576 - mae: 22.4646 - customMetric: 84201.7578\n",
      "Epoch 56/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4454.1602 - mae: 22.1570 - customMetric: 74063.6797\n",
      "Epoch 57/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4413.4419 - mae: 22.3542 - customMetric: 82543.4375\n",
      "Epoch 58/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4375.9800 - mae: 21.7879 - customMetric: 72103.0234\n",
      "Epoch 59/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4404.7280 - mae: 22.2989 - customMetric: 77500.8203\n",
      "Epoch 60/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4263.9365 - mae: 21.8264 - customMetric: 82967.1797\n",
      "Epoch 61/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4404.0029 - mae: 21.9965 - customMetric: 71305.9766\n",
      "Epoch 62/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4448.5146 - mae: 22.6887 - customMetric: 81095.7422\n",
      "Epoch 63/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4477.3032 - mae: 22.4679 - customMetric: 97888.2578\n",
      "Epoch 64/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4391.9219 - mae: 22.3187 - customMetric: 94266.5547\n",
      "Epoch 65/300\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 4353.2422 - mae: 22.1575 - customMetric: 78994.0000\n",
      "Epoch 66/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4804.7983 - mae: 23.3919 - customMetric: 81776.5156\n",
      "Epoch 67/300\n",
      "31/31 [==============================] - 2s 67ms/step - loss: 4807.6558 - mae: 23.2429 - customMetric: 80570.9688\n",
      "Epoch 68/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 5131.8965 - mae: 24.2856 - customMetric: 100498.7344\n",
      "Epoch 69/300\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 4696.3115 - mae: 23.2325 - customMetric: 129208.4531\n",
      "Epoch 70/300\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 4396.5542 - mae: 22.5751 - customMetric: 88945.0938\n",
      "Epoch 71/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4379.2734 - mae: 22.2229 - customMetric: 72773.1562\n",
      "Epoch 72/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4683.0259 - mae: 22.9338 - customMetric: 76620.3203\n",
      "Epoch 73/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4467.8232 - mae: 22.6304 - customMetric: 80304.5625\n",
      "Epoch 74/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4328.7461 - mae: 22.1480 - customMetric: 58175.1211\n",
      "Epoch 75/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4385.0386 - mae: 22.4737 - customMetric: 52344.6289\n",
      "Epoch 76/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4442.9097 - mae: 22.2636 - customMetric: 76514.9219\n",
      "Epoch 77/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 5211.8701 - mae: 24.8742 - customMetric: 37238.0938\n",
      "Epoch 78/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4531.7334 - mae: 22.4717 - customMetric: 32664.1445\n",
      "Epoch 79/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4531.5068 - mae: 22.2447 - customMetric: 39189.3320\n",
      "Epoch 80/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4394.6030 - mae: 22.0679 - customMetric: 46475.1094\n",
      "Epoch 81/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4673.3096 - mae: 23.0599 - customMetric: 42830.0195\n",
      "Epoch 82/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 5081.0127 - mae: 23.4962 - customMetric: 51565.3594\n",
      "Epoch 83/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4496.8223 - mae: 22.2896 - customMetric: 58025.8359\n",
      "Epoch 84/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4441.6260 - mae: 22.5187 - customMetric: 64071.5508\n",
      "Epoch 85/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4338.2026 - mae: 22.2610 - customMetric: 62237.5000\n",
      "Epoch 86/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4261.4912 - mae: 22.1026 - customMetric: 57321.3125\n",
      "Epoch 87/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4277.4824 - mae: 21.9101 - customMetric: 52735.9141\n",
      "Epoch 88/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4288.0396 - mae: 22.1365 - customMetric: 52907.0547\n",
      "Epoch 89/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4252.4814 - mae: 21.9354 - customMetric: 59766.6914\n",
      "Epoch 90/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4277.0674 - mae: 21.9065 - customMetric: 57457.5547\n",
      "Epoch 91/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4204.8760 - mae: 22.0082 - customMetric: 67339.1797\n",
      "Epoch 92/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4250.7876 - mae: 21.8372 - customMetric: 61191.5898\n",
      "Epoch 93/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4541.5527 - mae: 23.0621 - customMetric: 62507.7188\n",
      "Epoch 94/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4617.2969 - mae: 23.2156 - customMetric: 65755.0234\n",
      "Epoch 95/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4383.3076 - mae: 22.5032 - customMetric: 66364.3594\n",
      "Epoch 96/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4218.4849 - mae: 21.6365 - customMetric: 65988.3203\n",
      "Epoch 97/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4893.9854 - mae: 23.2912 - customMetric: 92778.4453\n",
      "Epoch 98/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4442.6313 - mae: 22.6063 - customMetric: 66543.0547\n",
      "Epoch 99/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4329.5742 - mae: 22.2861 - customMetric: 75465.2656\n",
      "Epoch 100/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4374.0806 - mae: 22.3120 - customMetric: 77502.8672\n",
      "Epoch 101/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4227.2578 - mae: 21.9889 - customMetric: 53693.7109\n",
      "Epoch 102/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4166.5698 - mae: 21.8015 - customMetric: 61245.9531\n",
      "Epoch 103/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4200.6187 - mae: 21.8620 - customMetric: 62771.2891\n",
      "Epoch 104/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4141.0640 - mae: 21.7481 - customMetric: 62104.2148\n",
      "Epoch 105/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4137.3950 - mae: 21.6339 - customMetric: 64876.0039\n",
      "Epoch 106/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4179.3931 - mae: 21.8576 - customMetric: 65683.3203\n",
      "Epoch 107/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4060.5112 - mae: 21.4113 - customMetric: 58504.9648\n",
      "Epoch 108/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4170.7803 - mae: 21.6143 - customMetric: 72577.8438\n",
      "Epoch 109/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4298.5601 - mae: 22.1688 - customMetric: 69733.2969\n",
      "Epoch 110/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4289.6836 - mae: 22.0851 - customMetric: 70508.6484\n",
      "Epoch 111/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4390.0742 - mae: 22.3367 - customMetric: 58527.1289\n",
      "Epoch 112/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4627.7603 - mae: 23.0758 - customMetric: 69479.0859\n",
      "Epoch 113/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4583.7749 - mae: 22.9567 - customMetric: 67309.4609\n",
      "Epoch 114/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4502.1348 - mae: 22.6921 - customMetric: 72575.5078\n",
      "Epoch 115/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4143.4258 - mae: 21.5434 - customMetric: 71638.1797\n",
      "Epoch 116/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4175.8809 - mae: 21.8398 - customMetric: 69160.7891\n",
      "Epoch 117/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4084.6204 - mae: 21.3861 - customMetric: 65233.6680\n",
      "Epoch 118/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4112.5879 - mae: 21.5295 - customMetric: 70912.8125\n",
      "Epoch 119/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4267.0093 - mae: 21.8781 - customMetric: 63587.7930\n",
      "Epoch 120/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4247.7588 - mae: 21.8394 - customMetric: 59702.8125\n",
      "Epoch 121/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4579.6621 - mae: 22.6046 - customMetric: 71676.7266\n",
      "Epoch 122/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4449.5908 - mae: 22.2252 - customMetric: 65314.9844\n",
      "Epoch 123/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4691.8657 - mae: 22.7582 - customMetric: 55722.0430\n",
      "Epoch 124/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 4330.2744 - mae: 22.3141 - customMetric: 48489.0508\n",
      "Epoch 125/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4208.4023 - mae: 22.0063 - customMetric: 72955.5625\n",
      "Epoch 126/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4088.5969 - mae: 21.8698 - customMetric: 90636.2891\n",
      "Epoch 127/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4013.0679 - mae: 21.2593 - customMetric: 61239.1797\n",
      "Epoch 128/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 5253.5728 - mae: 24.4057 - customMetric: 108087.7422\n",
      "Epoch 129/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 5177.3213 - mae: 24.6940 - customMetric: 98520.0156\n",
      "Epoch 130/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4715.3203 - mae: 23.1147 - customMetric: 87351.2266\n",
      "Epoch 131/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4437.7119 - mae: 22.5295 - customMetric: 76452.0312\n",
      "Epoch 132/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4303.5562 - mae: 22.4652 - customMetric: 81672.7578\n",
      "Epoch 133/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4164.3481 - mae: 21.7878 - customMetric: 66708.2109\n",
      "Epoch 134/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4056.7585 - mae: 21.5065 - customMetric: 54351.9453\n",
      "Epoch 135/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3985.7930 - mae: 21.3445 - customMetric: 58240.0664\n",
      "Epoch 136/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4027.5095 - mae: 21.2794 - customMetric: 54187.8242\n",
      "Epoch 137/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3932.0044 - mae: 21.2213 - customMetric: 56283.3477\n",
      "Epoch 138/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4095.3181 - mae: 21.3610 - customMetric: 58229.4180\n",
      "Epoch 139/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3958.3625 - mae: 21.0806 - customMetric: 62765.1055\n",
      "Epoch 140/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 4052.1077 - mae: 21.1104 - customMetric: 65095.3867\n",
      "Epoch 141/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 5225.4233 - mae: 23.2284 - customMetric: 46575.6836\n",
      "Epoch 142/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 5430.2432 - mae: 25.0682 - customMetric: 81549.3438\n",
      "Epoch 143/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 5098.0991 - mae: 24.3813 - customMetric: 115624.6719\n",
      "Epoch 144/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4269.4360 - mae: 22.3635 - customMetric: 88519.4609\n",
      "Epoch 145/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4245.4800 - mae: 22.0503 - customMetric: 73896.6719\n",
      "Epoch 146/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4235.2715 - mae: 22.0951 - customMetric: 74116.0156\n",
      "Epoch 147/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4231.8252 - mae: 22.0319 - customMetric: 84708.7734\n",
      "Epoch 148/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4000.4192 - mae: 21.3920 - customMetric: 63968.4609\n",
      "Epoch 149/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3932.9812 - mae: 21.0553 - customMetric: 65906.8359\n",
      "Epoch 150/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4031.5208 - mae: 21.2988 - customMetric: 68112.3594\n",
      "Epoch 151/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 3906.9834 - mae: 21.0782 - customMetric: 69828.5234\n",
      "Epoch 152/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 3891.5969 - mae: 20.5234 - customMetric: 58933.9570\n",
      "Epoch 153/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3841.4094 - mae: 20.9418 - customMetric: 66269.8984\n",
      "Epoch 154/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3855.5303 - mae: 21.0895 - customMetric: 61995.7539\n",
      "Epoch 155/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3943.7795 - mae: 21.0551 - customMetric: 64842.0781\n",
      "Epoch 156/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 4151.3481 - mae: 21.7483 - customMetric: 66790.9766\n",
      "Epoch 157/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4116.9106 - mae: 21.7261 - customMetric: 70009.2266\n",
      "Epoch 158/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 4009.0078 - mae: 21.3235 - customMetric: 63687.6914\n",
      "Epoch 159/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4113.8936 - mae: 21.6837 - customMetric: 71564.3594\n",
      "Epoch 160/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4067.3450 - mae: 21.3740 - customMetric: 78226.7188\n",
      "Epoch 161/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4084.7849 - mae: 21.8428 - customMetric: 67378.0469\n",
      "Epoch 162/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4020.0317 - mae: 21.3772 - customMetric: 61919.7656\n",
      "Epoch 163/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4031.0100 - mae: 21.5295 - customMetric: 51792.6602\n",
      "Epoch 164/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3968.9067 - mae: 21.0045 - customMetric: 57465.5703\n",
      "Epoch 165/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4012.1677 - mae: 21.5124 - customMetric: 72050.1953\n",
      "Epoch 166/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4024.2063 - mae: 21.3093 - customMetric: 75724.3828\n",
      "Epoch 167/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 3902.7510 - mae: 21.0767 - customMetric: 61741.6367\n",
      "Epoch 168/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 3965.3176 - mae: 21.0997 - customMetric: 67516.3203\n",
      "Epoch 169/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3991.3503 - mae: 21.6966 - customMetric: 54255.2891\n",
      "Epoch 170/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 4005.0474 - mae: 21.4342 - customMetric: 55518.2109\n",
      "Epoch 171/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 3972.8477 - mae: 21.4682 - customMetric: 53516.4922\n",
      "Epoch 172/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4009.6953 - mae: 21.2052 - customMetric: 57542.2383\n",
      "Epoch 173/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4593.1162 - mae: 22.5958 - customMetric: 95731.9844\n",
      "Epoch 174/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4077.3684 - mae: 21.4507 - customMetric: 69016.8594\n",
      "Epoch 175/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3789.4651 - mae: 20.7073 - customMetric: 68700.9844\n",
      "Epoch 176/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3720.3120 - mae: 20.7409 - customMetric: 67489.2734\n",
      "Epoch 177/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3705.2485 - mae: 20.1592 - customMetric: 62890.0391\n",
      "Epoch 178/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3811.7102 - mae: 20.7858 - customMetric: 67123.5547\n",
      "Epoch 179/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3704.6841 - mae: 20.2172 - customMetric: 65827.7891\n",
      "Epoch 180/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 3861.2756 - mae: 21.2565 - customMetric: 65701.7734\n",
      "Epoch 181/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4137.8882 - mae: 21.0241 - customMetric: 69624.1172\n",
      "Epoch 182/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 4484.8296 - mae: 22.5609 - customMetric: 98789.5547\n",
      "Epoch 183/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4679.4092 - mae: 22.7202 - customMetric: 103061.6641\n",
      "Epoch 184/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4010.5352 - mae: 21.3213 - customMetric: 79841.9922\n",
      "Epoch 185/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3803.1108 - mae: 20.7619 - customMetric: 75822.8672\n",
      "Epoch 186/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3762.5781 - mae: 20.3829 - customMetric: 68578.7891\n",
      "Epoch 187/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3794.6016 - mae: 20.6309 - customMetric: 80767.1328\n",
      "Epoch 188/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3716.4207 - mae: 20.9483 - customMetric: 81232.8438\n",
      "Epoch 189/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4699.7471 - mae: 22.1391 - customMetric: 60688.6328\n",
      "Epoch 190/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 4280.5454 - mae: 21.5945 - customMetric: 74436.9219\n",
      "Epoch 191/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3855.2537 - mae: 20.8891 - customMetric: 71947.3281\n",
      "Epoch 192/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 5409.5605 - mae: 23.0518 - customMetric: 74634.4844\n",
      "Epoch 193/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4929.6875 - mae: 22.7261 - customMetric: 100142.4297\n",
      "Epoch 194/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4696.6851 - mae: 22.0913 - customMetric: 89416.8047\n",
      "Epoch 195/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4550.8413 - mae: 21.3857 - customMetric: 91863.7578\n",
      "Epoch 196/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 4415.3657 - mae: 21.1037 - customMetric: 80974.8281\n",
      "Epoch 197/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4386.7769 - mae: 21.4243 - customMetric: 85301.1875\n",
      "Epoch 198/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4371.5488 - mae: 21.1518 - customMetric: 78202.3594\n",
      "Epoch 199/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4387.1958 - mae: 21.0806 - customMetric: 82471.6406\n",
      "Epoch 200/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4303.7417 - mae: 20.9928 - customMetric: 80656.8125\n",
      "Epoch 201/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4367.9292 - mae: 21.2507 - customMetric: 82518.0625\n",
      "Epoch 202/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4386.6475 - mae: 21.5700 - customMetric: 86418.2422\n",
      "Epoch 203/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4412.7314 - mae: 21.7770 - customMetric: 86615.0938\n",
      "Epoch 204/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4274.7847 - mae: 21.7252 - customMetric: 90504.8594\n",
      "Epoch 205/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4064.1294 - mae: 21.2187 - customMetric: 69485.5156\n",
      "Epoch 206/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4004.4441 - mae: 20.9132 - customMetric: 69112.9922\n",
      "Epoch 207/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4106.8755 - mae: 21.6000 - customMetric: 81886.1797\n",
      "Epoch 208/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3958.2747 - mae: 20.7424 - customMetric: 68079.4219\n",
      "Epoch 209/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3943.2888 - mae: 21.1010 - customMetric: 70388.4062\n",
      "Epoch 210/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4012.7068 - mae: 21.0404 - customMetric: 78559.1172\n",
      "Epoch 211/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3953.5994 - mae: 21.0114 - customMetric: 78284.9062\n",
      "Epoch 212/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4093.2583 - mae: 21.5064 - customMetric: 100507.5625\n",
      "Epoch 213/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4068.1584 - mae: 21.4458 - customMetric: 81799.4375\n",
      "Epoch 214/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 3900.7881 - mae: 20.9262 - customMetric: 71615.3672\n",
      "Epoch 215/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4266.2617 - mae: 22.2663 - customMetric: 70682.1875\n",
      "Epoch 216/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 4002.7644 - mae: 21.2338 - customMetric: 49125.4688\n",
      "Epoch 217/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3855.7874 - mae: 21.3042 - customMetric: 69641.1406\n",
      "Epoch 218/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3998.3118 - mae: 21.9225 - customMetric: 77534.6406\n",
      "Epoch 219/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4425.6162 - mae: 22.9516 - customMetric: 69177.8828\n",
      "Epoch 220/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 4298.8784 - mae: 22.6747 - customMetric: 69035.7656\n",
      "Epoch 221/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3934.6672 - mae: 21.6660 - customMetric: 65788.8984\n",
      "Epoch 222/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 3821.8232 - mae: 21.1242 - customMetric: 72919.8906\n",
      "Epoch 223/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 3636.7732 - mae: 20.7391 - customMetric: 65147.8086\n",
      "Epoch 224/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3584.7939 - mae: 20.2851 - customMetric: 58158.8477\n",
      "Epoch 225/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3562.8477 - mae: 20.1311 - customMetric: 63676.1680\n",
      "Epoch 226/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 3795.9421 - mae: 20.8061 - customMetric: 70888.9688\n",
      "Epoch 227/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3644.7527 - mae: 20.2829 - customMetric: 52984.2656\n",
      "Epoch 228/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3603.9846 - mae: 20.7120 - customMetric: 71199.7109\n",
      "Epoch 229/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3638.4258 - mae: 20.6052 - customMetric: 60115.5664\n",
      "Epoch 230/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 3625.7412 - mae: 20.4809 - customMetric: 58876.1719\n",
      "Epoch 231/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3656.0166 - mae: 20.5176 - customMetric: 62104.2969\n",
      "Epoch 232/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4067.2039 - mae: 22.0038 - customMetric: 79133.0078\n",
      "Epoch 233/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 3833.9668 - mae: 21.2800 - customMetric: 54261.0234\n",
      "Epoch 234/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 3676.5979 - mae: 20.6920 - customMetric: 72118.5547\n",
      "Epoch 235/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3637.0808 - mae: 20.6386 - customMetric: 78750.6250\n",
      "Epoch 236/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3696.1072 - mae: 20.5094 - customMetric: 75787.2188\n",
      "Epoch 237/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 3671.7610 - mae: 20.2106 - customMetric: 64865.3281\n",
      "Epoch 238/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 3798.8835 - mae: 20.5435 - customMetric: 62560.2891\n",
      "Epoch 239/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3747.2056 - mae: 20.8196 - customMetric: 79795.3672\n",
      "Epoch 240/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 3820.0459 - mae: 20.5611 - customMetric: 57613.2773\n",
      "Epoch 241/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4035.2708 - mae: 20.7349 - customMetric: 59969.6602\n",
      "Epoch 242/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4691.8223 - mae: 23.0106 - customMetric: 73802.1953\n",
      "Epoch 243/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4533.1250 - mae: 22.8726 - customMetric: 93176.7500\n",
      "Epoch 244/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3817.5647 - mae: 20.8610 - customMetric: 76066.0469\n",
      "Epoch 245/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 3733.9126 - mae: 20.9341 - customMetric: 62624.8008\n",
      "Epoch 246/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 3512.7542 - mae: 20.3297 - customMetric: 65762.9141\n",
      "Epoch 247/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3465.5037 - mae: 19.9293 - customMetric: 62445.6328\n",
      "Epoch 248/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 3443.2905 - mae: 19.9785 - customMetric: 63063.4219\n",
      "Epoch 249/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3436.0105 - mae: 19.7088 - customMetric: 57475.8125\n",
      "Epoch 250/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 3414.8105 - mae: 19.8182 - customMetric: 66150.8516\n",
      "Epoch 251/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3394.1172 - mae: 19.5112 - customMetric: 66079.9531\n",
      "Epoch 252/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3460.1460 - mae: 19.7459 - customMetric: 63816.3633\n",
      "Epoch 253/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 3509.3591 - mae: 19.9852 - customMetric: 61690.2656\n",
      "Epoch 254/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3734.7690 - mae: 20.5245 - customMetric: 88972.4219\n",
      "Epoch 255/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3560.4771 - mae: 19.9345 - customMetric: 65051.0938\n",
      "Epoch 256/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 3525.7981 - mae: 20.1576 - customMetric: 60353.8828\n",
      "Epoch 257/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3685.6289 - mae: 20.8124 - customMetric: 68441.0781\n",
      "Epoch 258/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3521.2449 - mae: 20.0454 - customMetric: 59120.5000\n",
      "Epoch 259/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 3610.9197 - mae: 20.6340 - customMetric: 73147.7500\n",
      "Epoch 260/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3695.2104 - mae: 20.4242 - customMetric: 70261.9297\n",
      "Epoch 261/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4564.0479 - mae: 22.4607 - customMetric: 109122.1016\n",
      "Epoch 262/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 4190.8296 - mae: 22.2575 - customMetric: 95169.9766\n",
      "Epoch 263/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3769.5886 - mae: 21.1834 - customMetric: 76256.7891\n",
      "Epoch 264/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3727.6990 - mae: 20.8904 - customMetric: 82702.5938\n",
      "Epoch 265/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3580.9998 - mae: 20.3241 - customMetric: 71147.2031\n",
      "Epoch 266/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3446.7529 - mae: 20.0308 - customMetric: 60245.0859\n",
      "Epoch 267/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 3399.4263 - mae: 19.4828 - customMetric: 61718.8320\n",
      "Epoch 268/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3379.2949 - mae: 19.7068 - customMetric: 68892.6641\n",
      "Epoch 269/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3384.0342 - mae: 19.5931 - customMetric: 61892.8867\n",
      "Epoch 270/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3534.9536 - mae: 19.9787 - customMetric: 68640.6484\n",
      "Epoch 271/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3439.9309 - mae: 19.7334 - customMetric: 66126.9844\n",
      "Epoch 272/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3338.4382 - mae: 19.3688 - customMetric: 60180.0977\n",
      "Epoch 273/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3365.8577 - mae: 19.6935 - customMetric: 64431.5742\n",
      "Epoch 274/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 3347.9155 - mae: 19.8165 - customMetric: 62410.2383\n",
      "Epoch 275/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 3299.3186 - mae: 19.3901 - customMetric: 67054.7578\n",
      "Epoch 276/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 3293.2617 - mae: 19.4702 - customMetric: 62280.1523\n",
      "Epoch 277/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3309.3965 - mae: 19.3368 - customMetric: 62188.6211\n",
      "Epoch 278/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3280.3796 - mae: 19.3020 - customMetric: 62263.6055\n",
      "Epoch 279/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3284.3203 - mae: 19.1814 - customMetric: 53875.5352\n",
      "Epoch 280/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3342.3208 - mae: 19.5415 - customMetric: 60411.9297\n",
      "Epoch 281/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3251.9922 - mae: 19.2000 - customMetric: 62103.3164\n",
      "Epoch 282/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3265.1013 - mae: 19.0393 - customMetric: 60827.1719\n",
      "Epoch 283/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3240.3210 - mae: 19.3077 - customMetric: 66620.8828\n",
      "Epoch 284/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3249.0425 - mae: 19.0494 - customMetric: 58089.6133\n",
      "Epoch 285/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3242.9233 - mae: 19.0939 - customMetric: 60459.1758\n",
      "Epoch 286/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3352.5007 - mae: 19.5335 - customMetric: 63741.2148\n",
      "Epoch 287/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3283.9939 - mae: 19.3910 - customMetric: 57167.1484\n",
      "Epoch 288/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3693.1196 - mae: 20.0076 - customMetric: 56896.5312\n",
      "Epoch 289/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4390.9766 - mae: 22.2197 - customMetric: 66502.3281\n",
      "Epoch 290/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4428.3599 - mae: 21.5666 - customMetric: 54073.9961\n",
      "Epoch 291/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 4065.4844 - mae: 21.2004 - customMetric: 102256.8047\n",
      "Epoch 292/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3818.5352 - mae: 21.2744 - customMetric: 63009.2227\n",
      "Epoch 293/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3581.1604 - mae: 20.9163 - customMetric: 72934.2656\n",
      "Epoch 294/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3386.2112 - mae: 19.7762 - customMetric: 53240.7227\n",
      "Epoch 295/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3299.4463 - mae: 19.5902 - customMetric: 57338.0469\n",
      "Epoch 296/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3771.8545 - mae: 20.4261 - customMetric: 80762.0156\n",
      "Epoch 297/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3391.3027 - mae: 19.6784 - customMetric: 62951.3828\n",
      "Epoch 298/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 3562.9094 - mae: 20.2430 - customMetric: 62098.3086\n",
      "Epoch 299/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 4260.0630 - mae: 22.1209 - customMetric: 55236.0039\n",
      "Epoch 300/300\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 4491.6660 - mae: 23.0332 - customMetric: 49850.6562\n"
     ]
    }
   ],
   "source": [
    "H_t = []\n",
    "index = 0\n",
    "\n",
    "# Using custom loss and gen\n",
    "es = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.01, patience=25, mode='min', verbose=1, restore_best_weights=True)\n",
    "# H = model.fit(x=x_train, y=y_train, batch_size=64, epochs=300, validation_data=(x_test, y_test), verbose=1, callbacks=[es])\n",
    "# H = model.fit(x=x_train, y=y_train, batch_size=64, epochs=300, shuffle=True, verbose=1, callbacks=[es])\n",
    "# H = model.fit(x=x_train, y=y_train[:,2], batch_size=256, epochs=300, verbose=1, shuffle=True, callbacks=[es])\n",
    "# H_t.append(model.fit(x=x_train, y=y_train[:,2], batch_size=256, epochs=300, verbose=1, shuffle=True, callbacks=[es]).history)\n",
    "# H_t.append(model.fit(x=x_train, y=y_train[:,index], batch_size=256, epochs=300, verbose=1, shuffle=True, callbacks=[es]).history)\n",
    "H_t.append(model.fit(x=x_train, y=y_train, batch_size=256, epochs=300, verbose=1, shuffle=True).history)\n",
    "# model.fit(x=[x_train[0],x_train[1],x_train[2],np.expand_dims(x_train[3][:,index],-1)], y=y_train[:,index], batch_size=256, epochs=300, verbose=1, shuffle=True)\n",
    "if H == None:\n",
    "  H = H_t[-1]\n",
    "else:\n",
    "  H = concat_hist(H,H_t[-1])\n",
    "# H = model.fit(x=x_train, y=y_train, batch_size=64, epochs=100, validation_data=test_gen, validation_steps=50, validation_batch_size=32, verbose=1)\n",
    "\n",
    "# Example how it kind of looks like\n",
    "# H = model.fit(x=[x_train, invCov, y_train], y=y_train, batch_size=64, epochs=100, verbose=1)\n",
    "\n",
    "# Overfit\n",
    "# es = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.001, patience=100, mode='min', verbose=1, restore_best_weights=True)\n",
    "# H = model.fit(x=x_train, y=y_train, batch_size=1, epochs=100, verbose=1, callbacks=[es])\n",
    "# H = model.fit(x=x_train, y=y_train, batch_size=1, epochs=100, verbose=1, validation_data=(x_test,y_test), callbacks=[es])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZ5XBwDV7MGY"
   },
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_index = 0\n",
    "date = \"03-07\"\n",
    "final_loss = 0\n",
    "def gen_name(m_type):\n",
    "    global model_index, index, date, final_loss\n",
    "    variable = [\"q_pt\",\"phi\",\"tanl\",\"D\",\"z\"][index]\n",
    "    m_str = \"models/\" + str(date) + \"-2021_\" + str(variable) + \"-\" + str(model_index) + \"_loss=\" + str(final_loss) + \".\" + str(m_type)\n",
    "    model_index+=1 # create a check for if file exists, for now just increment\n",
    "    return m_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWvST6o27MYI"
   },
   "outputs": [],
   "source": [
    "# model.save('model.h5', save_format=\"h5\")\n",
    "# TODO check if file exists, increment counter\n",
    "# model.save('drive/MyDrive/Models/RealRNN_1-3-2021_141Ep_Onlytanl-2.h5', save_format=\"h5\")\n",
    "final_loss = round(H[\"loss\"][-1],2)\n",
    "model.save(gen_name(\"h5\"), save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6LGBSia7fg3"
   },
   "source": [
    "## Graph loss and mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "id": "OUPAStLMtq7m",
    "outputId": "d4bada3c-6f91-4a9c-b6d7-d7abe8cc1689"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'mae', 'customMetric'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(H.keys())\n",
    "# print(\"loss: \", H[\"loss\"])\n",
    "# print(\"mae: \", H[\"mae\"])\n",
    "# print(\"val_loss: \", H.history[\"val_loss\"])\n",
    "# print(\"val_mae: \", H.history[\"val_mae\"])\n",
    "\n",
    "lim = 0\n",
    "\n",
    "fig, ax = plt.subplots(2,1,figsize=(5,10))\n",
    "fig.subplots_adjust(hspace=0.35)\n",
    "\n",
    "ax[0].plot(H[\"loss\"][lim:])\n",
    "# ax[0].plot(H.history[\"val_loss\"][lim:])\n",
    "ax[0].set_title(\"loss vs epoch\", fontsize=20)\n",
    "ax[0].set_xlabel(\"epoch\", fontsize=15)\n",
    "ax[0].set_ylabel(\"loss\", fontsize=15)\n",
    "ax[0].legend([\"train\",\"val\"])\n",
    "ax[0].grid(True)\n",
    "\n",
    "\n",
    "ax[1].plot(H[\"mae\"][lim:])\n",
    "# ax[1].plot(H.history[\"val_mae\"][lim:])\n",
    "ax[1].set_title(\"mae vs epoch\", fontsize=20)\n",
    "ax[1].set_xlabel(\"epoch\", fontsize=15)\n",
    "ax[1].set_ylabel(\"mae\", fontsize=15)\n",
    "ax[1].legend([\"train\",\"val\"])\n",
    "ax[1].grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_index -= 1\n",
    "plt.savefig(gen_name(\"png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an_array = np.array(list(H.items()))\n",
    "save_array = np.array([an_array[0][1],an_array[1][1]])\n",
    "model_index -= 1\n",
    "np.save(gen_name(\"npy\"),save_array,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xw4xzpuKYEld"
   },
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kZHLFMS1YE40",
    "outputId": "09e30fe4-cc1b-444f-83b3-c74d1b923762"
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True, precision=3)\n",
    "from time import sleep\n",
    "\n",
    "for i in range(10):\n",
    "  aax = x_train[0][i:i+1]\n",
    "  oax = x_train[1][i:i+1]\n",
    "  cax = x_train[2][i:i+1]\n",
    "  yax = x_train[3][i:i+1]\n",
    "  aay = y_train[i][index]\n",
    "  pred = model.predict([aax, oax, cax, yax])[0][0]\n",
    "  diff = pred - aay\n",
    "  # print(\"i: \",i)\n",
    "  # print(\"aax:  \",aax[0,0])\n",
    "  print(\"aay:  \",aay)\n",
    "  print(\"pred: \",pred)\n",
    "  # print(\"diff: \",diff)\n",
    "  print(\"\")\n",
    "\n",
    "  # plt.plot(aay[0])\n",
    "  # plt.plot(pred[0])\n",
    "  # plt.show()\n",
    "# [Q/PT, phi, tanl, D, z]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T56J0X6g7O2k"
   },
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zgLQh0jf7Ova"
   },
   "outputs": [],
   "source": [
    "# model.save('drive/MyDrive/RealRNN_1-2-2021_2.h5', save_format=\"h5\")\n",
    "model = keras.models.load_model('drive/MyDrive/Models/RealRNN_1-3-2021_300Ep_Onlyphi.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X72YH9S87cvW"
   },
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(H.keys())\n",
    "# print(\"loss: \", H[\"loss\"])\n",
    "# print(\"mae: \", H[\"mae\"])\n",
    "# print(\"val_loss: \", H.history[\"val_loss\"])\n",
    "# print(\"val_mae: \", H.history[\"val_mae\"])\n",
    "\n",
    "lim = 0\n",
    "\n",
    "if len(H.keys()) > 4:\n",
    "  # fig, ax = plt.subplots(4,1,figsize=(5,20))\n",
    "  fig, ax = plt.subplots(3,1,figsize=(5,15))\n",
    "  fig.subplots_adjust(hspace=0.35)\n",
    "\n",
    "  ax[0].plot(H[\"loss\"][lim:])\n",
    "  # ax[0].plot(H.history[\"val_loss\"][lim:])\n",
    "  ax[0].set_title(\"loss vs epoch\", fontsize=20)\n",
    "  ax[0].set_xlabel(\"epoch\", fontsize=15)\n",
    "  ax[0].set_ylabel(\"loss\", fontsize=15)\n",
    "  # ax[0].set_yscale(\"log\")\n",
    "  ax[0].legend([\"train\",\"val\"])\n",
    "  ax[0].grid(True)\n",
    "\n",
    "\n",
    "  ax[1].plot(H[\"mae\"][lim:])\n",
    "  # ax[1].plot(H.history[\"val_mae\"][lim:])\n",
    "  ax[1].set_title(\"mae vs epoch\", fontsize=20)\n",
    "  ax[1].set_xlabel(\"epoch\", fontsize=15)\n",
    "  ax[1].set_ylabel(\"mae\", fontsize=15)\n",
    "  ax[1].legend([\"train\",\"val\"])\n",
    "  ax[1].grid(True)\n",
    "\n",
    "  ax[2].plot(H[\"q_pt\"][lim:])\n",
    "  ax[2].plot(H[\"phi\"][lim:])\n",
    "  ax[2].plot(H[\"tanl\"][lim:])\n",
    "  ax[2].plot(H[\"D\"][lim:])\n",
    "  ax[2].plot(H[\"z\"][lim:])\n",
    "  ax[2].set_title(\"data vs epoch\", fontsize=20)\n",
    "  ax[2].set_xlabel(\"epoch\", fontsize=15)\n",
    "  ax[2].set_ylabel(\"data\", fontsize=15)\n",
    "  ax[2].legend([\"q_pt\",\"phi\",\"tanl\",\"D\",\"z\"])\n",
    "  # ax[2].legend([\"phi\",\"tanl\",\"D\",\"z\"])\n",
    "  # ax[2].legend([\"phi\",\"D\",\"z\"])\n",
    "  ax[2].grid(True)\n",
    "\n",
    "  # ax[3].plot(H.history[\"val_q_pt\"][lim:])\n",
    "  # ax[3].plot(H.history[\"val_phi\"][lim:])\n",
    "  # ax[3].plot(H.history[\"val_tanl\"][lim:])\n",
    "  # ax[3].plot(H.history[\"val_D\"][lim:])\n",
    "  # ax[3].plot(H.history[\"val_z\"][lim:])\n",
    "  # ax[3].set_title(\"data vs epoch\", fontsize=20)\n",
    "  # ax[3].set_xlabel(\"epoch\", fontsize=15)\n",
    "  # ax[3].set_ylabel(\"data\", fontsize=15)\n",
    "  # ax[3].legend([\"val_q_pt\",\"val_phi\",\"val_tanl\",\"val_D\",\"val_z\"])\n",
    "  # # ax[3].legend([\"val_phi\",\"val_tanl\",\"val_D\",\"val_z\"])\n",
    "  # # ax[3].legend([\"val_phi\",\"val_D\",\"val_z\"])\n",
    "  # ax[3].grid(True)\n",
    "\n",
    "else:\n",
    "  fig, ax = plt.subplots(2,1,figsize=(5,10))\n",
    "  fig.subplots_adjust(hspace=0.35)\n",
    "\n",
    "  ax[0].plot(H[\"loss\"][lim:])\n",
    "  # ax[0].plot(H.history[\"val_loss\"][lim:])\n",
    "  ax[0].set_title(\"loss vs epoch\", fontsize=20)\n",
    "  ax[0].set_xlabel(\"epoch\", fontsize=15)\n",
    "  ax[0].set_ylabel(\"loss\", fontsize=15)\n",
    "  # ax[0].set_yscale(\"log\")\n",
    "  ax[0].legend([\"train\",\"val\"])\n",
    "  ax[0].grid(True)\n",
    "\n",
    "\n",
    "  ax[1].plot(H[\"mae\"][lim:])\n",
    "  # ax[1].plot(H.history[\"val_mae\"][lim:])\n",
    "  ax[1].set_title(\"mae vs epoch\", fontsize=20)\n",
    "  ax[1].set_xlabel(\"epoch\", fontsize=15)\n",
    "  ax[1].set_ylabel(\"mae\", fontsize=15)\n",
    "  ax[1].legend([\"train\",\"val\"])\n",
    "  ax[1].grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "id": "k6XSgSit7dE3",
    "outputId": "b3a1caaf-1b21-4348-bfe7-1315ccb39171"
   },
   "outputs": [],
   "source": [
    "print(H.history.keys())\n",
    "print(\"loss: \", H.history[\"loss\"])\n",
    "print(\"mae: \", H.history[\"mae\"])\n",
    "# print(\"val_loss: \", H.history[\"val_loss\"])\n",
    "# print(\"val_mae: \", H.history[\"val_mae\"])\n",
    "\n",
    "lim = 2\n",
    "\n",
    "fig, ax = plt.subplots(2,1,figsize=(5,10))\n",
    "fig.subplots_adjust(hspace=0.35)\n",
    "\n",
    "ax[0].plot(H.history[\"loss\"][lim:])\n",
    "# ax[0].plot(H.history[\"val_loss\"][lim:])\n",
    "ax[0].set_title(\"loss vs epoch\", fontsize=20)\n",
    "ax[0].set_xlabel(\"epoch\", fontsize=15)\n",
    "ax[0].set_ylabel(\"loss\", fontsize=15)\n",
    "ax[0].set_yscale(\"log\")\n",
    "ax[0].legend([\"train\",\"val\"])\n",
    "ax[0].grid(True)\n",
    "\n",
    "\n",
    "ax[1].plot(H.history[\"mae\"][lim:])\n",
    "# ax[1].plot(H.history[\"val_mae\"][lim:])\n",
    "ax[1].set_title(\"mae vs epoch\", fontsize=20)\n",
    "ax[1].set_xlabel(\"epoch\", fontsize=15)\n",
    "ax[1].set_ylabel(\"mae\", fontsize=15)\n",
    "ax[1].legend([\"train\",\"val\"])\n",
    "ax[1].grid(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UV GRAPHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0][0].shape[0] # (event,hits,18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "for z in range(10):\n",
    "    col = 1\n",
    "    row = 2\n",
    "#     fig, ax = plt.subplots(row,col,figsize=(5*col,5*row))\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    gs = fig.add_gridspec(row, col, hspace=0, wspace=0)\n",
    "    ax = gs.subplots(sharex='col', sharey='row')\n",
    "    \n",
    "#     fig.subplots_adjust(hspace=0.5)\n",
    "    u_hits = []\n",
    "    v_hits = []\n",
    "    z_hits = []\n",
    "    for i in range(x_train[0][z].shape[0]):\n",
    "        u_hits.append(x_train[0][z][i][0])\n",
    "        v_hits.append(x_train[0][z][i][1])\n",
    "        z_hits.append(x_train[0][z][i][6])\n",
    "    ax[0].plot(z_hits,u_hits,\"o\")\n",
    "#     ax[0].x_label(\"z_hits\")\n",
    "#     ax[0].y_label(\"u_hits\")\n",
    "    ax[1].plot(z_hits,v_hits,\"o\")\n",
    "#     ax[1].x_label(\"z_hits\")\n",
    "#     ax[1].x_label(\"v_hits\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOi6MC8u7swr"
   },
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pqqb7Riq7tAY"
   },
   "outputs": [],
   "source": [
    "# Maybe copy over previous function and edit that?\n",
    "def graph(pred, true, diff):\n",
    "\n",
    "  values = [\"u\",\"v\",\"sin(v)\",\"cos(v)\",\"sin(u)\",\"cos(u)\",\"s\",\"ds\",\"wire\",\"glayer\",\"z\",\"time\",\"dE_amp\",\"q\"]\n",
    "  limits = [[\"todo\"]]\n",
    "\n",
    "  size = len(values)\n",
    "\n",
    "  fig, axs = plt.subplots(4,size,figsize=(size*5,20))\n",
    "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "\n",
    "  for i in range(size):\n",
    "    (mu, sigma) = norm.fit(diff[:,i])\n",
    "    print(\"data\" , values[i] ,\" |: mu: \", mu, \"sigma: \" , sigma)\n",
    "    _, bins, _ = axs[0,i].hist(diff[:,i], 20, density=True)\n",
    "    y = norm.pdf(bins, mu, sigma)\n",
    "    l = axs[0,i].plot(bins, y, 'r--', linewidth=2)\n",
    "\n",
    "    axs[0,i].set_title(values[i] + ' diff')\n",
    "    axs[0,i].set_ylabel('freq')\n",
    "    axs[0,i].set_xlabel(values[i] + ' diff')\n",
    "\n",
    "  #--------------------------------------\n",
    "  # PREDICTED VS TRUE\n",
    "  #--------------------------------------\n",
    "    \n",
    "  for i in range(size):\n",
    "    axs[1,i].scatter(true[:,i],pred[:,i])\n",
    "    axs[1,i].grid(True)\n",
    "\n",
    "    axs[1,i].set_title(values[i] + ' (predicted vs true)')\n",
    "    axs[1,i].set_ylabel('pred ' + values[i])\n",
    "    axs[1,i].set_xlabel('true ' + values[i])\n",
    "\n",
    "    # axs[1,i].set_xlim(limits[i])\n",
    "    # axs[1,i].set_ylim(limits[i])\n",
    "    # axs[1,i].plot(limits[i],limits[i], color='b')\n",
    "\n",
    "  #--------------------------------------\n",
    "  # DIFFERENCE VS TRUE\n",
    "  #--------------------------------------\n",
    "\n",
    "  for i in range(size):\n",
    "    axs[2,i].scatter(true[:,i],diff[:,i])\n",
    "    l, r = axs[2,i].get_xlim()\n",
    "    axs[2,i].hlines(0, l, r)\n",
    "    axs[2,i].grid(True)\n",
    "\n",
    "    axs[2,i].set_title(values[i] + ' (difference vs true)')\n",
    "    axs[2,i].set_ylabel('diff ' + values[i])\n",
    "    axs[2,i].set_xlabel('true ' + values[i])\n",
    "\n",
    "  #--------------------------------------\n",
    "  # DIFFERENCE VS TRUE 2D HIST\n",
    "  #--------------------------------------\n",
    "\n",
    "  for i in range(size):\n",
    "    axs[3,i].hist2d(true[:,i],diff[:,i],bins=20)\n",
    "\n",
    "    axs[2,i].set_title(values[i] + ' (difference vs true)')\n",
    "    axs[2,i].set_ylabel('diff ' + values[i])\n",
    "    axs[2,i].set_xlabel('true ' + values[i])\n",
    "\n",
    "  fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHuUmXPCiXt9"
   },
   "outputs": [],
   "source": [
    "def gen_test_data(x_test, y_test, size=1000):\n",
    "  pred = model.predict(x_test)\n",
    "  diff = pred - y_test\n",
    "  return pred, y_test, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2AMRhD6Wi7Xh"
   },
   "outputs": [],
   "source": [
    "graph(gen_test_data(x_test, y_test));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAUyMneo7Xj1"
   },
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBsHjTgG7X2y"
   },
   "outputs": [],
   "source": [
    "# make test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWkYEQvFZ7HC"
   },
   "source": [
    "# Verification of proper data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqKxKcOQYEO4"
   },
   "source": [
    "## Using Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ml3HHzoEaAhr"
   },
   "outputs": [],
   "source": [
    "aax, aay = next(train_gen)\n",
    "print(aax.shape)\n",
    "print(aay.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gv72llYMaUYd"
   },
   "outputs": [],
   "source": [
    "print(\"x\",aax[0])\n",
    "print(\"y\",aay[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcwDKKLLe079"
   },
   "source": [
    "## Non Genenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DlU3tRGpYxQV",
    "outputId": "a24ded2d-d1d0-4555-84a9-428edf5b2e7d"
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "  aax = x_train[i]\n",
    "  aay = y_train[i]\n",
    "  # print(aax.shape)\n",
    "  # print(aay.shape)\n",
    "  # print(\"x\",aax)\n",
    "  print(\"y\",aay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCpP2wsfdeGl"
   },
   "source": [
    "## Graphs of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAxLbw94GYzm"
   },
   "source": [
    "### filter_ignore\n",
    "\n",
    "Filters out large and small values and graphs them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FiUKfbtfAoLm"
   },
   "outputs": [],
   "source": [
    "def filter_ignore(var,min=None,max=None,bins=25,ylog=False,xlog=False,cut=True):\n",
    "  list_ignore = []\n",
    "\n",
    "  print(\"--== {} ==--\\n\".format(var))\n",
    "\n",
    "  largest = 0\n",
    "  smallest = 0\n",
    "  for i in range(len(csv_train[var])):\n",
    "    if csv_train[var][i] > csv_train[var][largest]:\n",
    "      largest = i\n",
    "    if csv_train[var][i] < csv_train[var][smallest]:\n",
    "      smallest = i\n",
    "  print(\"largest value:  ({}, {:.3f})\".format(largest,csv_train[var][largest]))\n",
    "  print(\"smallest value: ({}, {:.3f})\".format(smallest,csv_train[var][smallest]))\n",
    "\n",
    "  print(\"\")\n",
    "\n",
    "  if min:\n",
    "    for i in range(len(csv_train[var])):\n",
    "      if csv_train[var][i] < min:\n",
    "        list_ignore.append(i)\n",
    "    print(\"min IDs to ignore for '{}':\".format(var))\n",
    "    print(csv_train[var][list_ignore])\n",
    "    print(\"\")\n",
    "  if max:\n",
    "    for i in range(len(csv_train[var])):\n",
    "      if csv_train[var][i] > max:\n",
    "        list_ignore.append(i)\n",
    "    print(\"max IDs to ignore for '{}':\".format(var))\n",
    "    print(csv_train[var][list_ignore])\n",
    "    print(\"\")\n",
    "  if min and max:\n",
    "    print(\"total IDs to ignore for '{}':\".format(var))\n",
    "    print(csv_train[var][list_ignore])\n",
    "    print(\"\")\n",
    "    plt.hist(csv_train[var],range=[min,max],bins=bins)\n",
    "  elif min:\n",
    "    plt.hist(csv_train[var],range=[min,csv_train[var][largest]],bins=bins)\n",
    "  elif max:\n",
    "    plt.hist(csv_train[var],range=[csv_train[var][smallest],max],bins=bins)\n",
    "  else:\n",
    "    plt.hist(csv_train[var],bins=bins)\n",
    "  \n",
    "  plt.title(var)\n",
    "  if cut:\n",
    "    plt.xlim(left=min,right=max)\n",
    "  if ylog:\n",
    "    plt.yscale(\"log\")\n",
    "  if xlog:\n",
    "    plt.xscale(\"log\")\n",
    "  plt.show()\n",
    "  return list_ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "BwsJ0YIoBiew",
    "outputId": "962724f0-0845-4ab5-8b27-043de6744e44"
   },
   "outputs": [],
   "source": [
    "filter_ignore(\"q_over_pt\",min=-4000,bins=30,ylog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "2Wyjq07YGjBF",
    "outputId": "9a82db49-7e7e-49aa-8d4b-a0da738ec1f5"
   },
   "outputs": [],
   "source": [
    "filter_ignore(\"tanl\",max=1000,bins=25,ylog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xauTTOyrHPyT"
   },
   "outputs": [],
   "source": [
    "rms_ignore = filter_ignore(\"rms\",max=0.1,bins=25,ylog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "ERa-RZDbN6Hm",
    "outputId": "ac4577c9-f719-47d4-bc16-8fdbb7e413a5"
   },
   "outputs": [],
   "source": [
    "# 'q_over_pt', 'phi', 'tanl', 'D', 'z'\n",
    "# filter_ignore(\"D\", min=-200, ylog=True,bins=25)\n",
    "filter_ignore(\"z\",bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UBO9wKCH2ePH"
   },
   "outputs": [],
   "source": [
    "csv_train.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yc3WNfPaYwaL"
   },
   "source": [
    "### 1D Hist of all Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 809
    },
    "id": "2VqlQxZRgTB7",
    "outputId": "2a5ce7d3-1665-4d71-80f5-5b7ed7ceb673"
   },
   "outputs": [],
   "source": [
    "plt.hist(csv_train[\"phi\"],bins=50) # -3 to 3, even distrib\n",
    "plt.title(\"phi\")\n",
    "plt.show()\n",
    "# ---\n",
    "plt.hist(csv_train[\"D\"],range=[-3000,80],bins=25) # -3000 to 50, but val in 65\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"D\")\n",
    "plt.show()\n",
    "# ---\n",
    "plt.hist(csv_train[\"z\"],bins=100)\n",
    "plt.title(\"z\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TbqYK55L05mB",
    "outputId": "81dc2a8d-b572-4505-983a-8958a9bea8b6"
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(2,1,figsize=(5,10))\n",
    "# fig.subplots_adjust(hspace=0.35)\n",
    "\n",
    "plt.hist(csv_train[\"cov_00\"],range=[0,1e8],bins=25) # 0 to 1e13\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"cov_00\")\n",
    "plt.show()\n",
    "# ---\n",
    "plt.hist(csv_train[\"cov_01\"],bins=25) # -1e6 to over 1e5\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"cov_01\")\n",
    "plt.show()\n",
    "# ---\n",
    "plt.hist(csv_train[\"chisq\"],bins=25) # 0 to 200\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"chisq\")\n",
    "plt.show()\n",
    "# ---\n",
    "plt.hist(csv_train[\"Ndof\"],range=[0,44],bins=45) # ? this one weird 0 to ~43\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Ndof\")\n",
    "plt.show()\n",
    "# ---\n",
    "plt.hist(csv_train[\"rms\"],range=[0,0.1],bins=25) # \n",
    "# plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"rms\")\n",
    "plt.show()\n",
    "# ---\n",
    "# plt.hist(csv_train[\"t_start_cntr\"],bins=25) # -60 to ~50\n",
    "plt.hist(csv_train[csv_train[\"t_start_cntr_valid\"] == 1][\"t_start_cntr\"],bins=25) # -60 to ~50\n",
    "plt.title(\"t_start_cntr\")\n",
    "plt.show()\n",
    "\n",
    "# plt.hist(csv_train[\"t_tof\"],bins=25) # ~-120 to ~175\n",
    "plt.hist(csv_train[csv_train[\"t_tof_valid\"] == 1][\"t_tof\"],bins=25) # ~-120 to ~175\n",
    "plt.title(\"t_tof\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"t_bcal\"],bins=25) # ~-22 to 20\n",
    "plt.title(\"t_bcal\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"t_fcal\"],bins=25) # ~-100 to ~75\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"t_fcal\")\n",
    "plt.show()\n",
    "# ---\n",
    "plt.hist(csv_train[\"t_start_cntr_valid\"],bins=25) # a lot more 0s\n",
    "plt.title(\"t_start_cntr_valid\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"t_tof_valid\"],bins=25) # about 5050\n",
    "plt.title(\"t_tof_valid\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"t_bcal_valid\"],bins=25) # almost all 0s\n",
    "plt.title(\"t_bcal_valid\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"t_fcal_valid\"],bins=25) # almost all 0s\n",
    "plt.title(\"t_fcal_valid\")\n",
    "plt.show()\n",
    "# ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbzN70C9MCrQ"
   },
   "source": [
    "### 1D Hist of Hit1 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VGzHLbKsLUZ9",
    "outputId": "0e1a0a14-8881-439d-c2f3-f0ec09006cc4"
   },
   "outputs": [],
   "source": [
    "plt.hist(csv_train[\"hit1_u\"],bins=25) # -42 to 42\n",
    "plt.title(\"hit1_u\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_v\"],bins=25) # -42 to 42\n",
    "plt.title(\"hit1_v\")\n",
    "plt.show()\n",
    "# plt.hist(csv_train[\"hit1_sinv\"],bins=25) # most are 0.96603 almost all are around that though\n",
    "# plt.title(\"hit1_sinv\")\n",
    "# plt.show()\n",
    "# plt.hist(csv_train[\"hit1_cosv\"],bins=25) # most -0.2585\n",
    "# plt.title(\"hit1_cosv\")\n",
    "# plt.show()\n",
    "# plt.hist(csv_train[\"hit1_sinu\"],bins=25) # most 0.96585\n",
    "# plt.title(\"hit1_sinu\")\n",
    "# plt.show()\n",
    "# plt.hist(csv_train[\"hit1_cosu\"],bins=25) # most 0.2591\n",
    "# plt.title(\"hit1_cosu\")\n",
    "# plt.show()\n",
    "plt.hist(csv_train[\"hit1_s\"],bins=25) # -42 to 42\n",
    "plt.title(\"hit1_s\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_ds\"],bins=25) # 0.01 to 0.04\n",
    "plt.title(\"hit1_ds\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_wire\"],bins=101,range=[0,100]) # 0 to 100\n",
    "plt.title(\"hit1_wire\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_glayer\"],bins=25,range=[0,26]) # 6 to 23\n",
    "plt.title(\"hit1_glayer\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_z\"],bins=25) # spaced out between 180 and 340\n",
    "plt.title(\"hit1_z\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_time\"],bins=25) # -75 to 270\n",
    "plt.title(\"hit1_time\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_dE_amp\"],bins=25) # 0 to 2e-7\n",
    "plt.title(\"hit1_dE_amp\")\n",
    "plt.show()\n",
    "plt.hist(csv_train[\"hit1_q\"],bins=25) # 0 to 85\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"hit1_q\")\n",
    "plt.show()\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxIRSBXpYzU7"
   },
   "source": [
    "### 2D Scatters of various data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "8hgtNdVxY-4_",
    "outputId": "9e12924e-9cb0-492c-fcbb-8cbc7ed4680e"
   },
   "outputs": [],
   "source": [
    "plt.scatter(csv_train[\"tanl\"],abs(csv_train[\"q_over_pt\"]),s=0.01) # a lot more 0s\n",
    "plt.title(\"q_over_pt vs tanl\")\n",
    "plt.xlabel(\"tanl\")\n",
    "plt.ylabel(\"q_over_pt\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "4pjOeJd85uWM",
    "outputId": "887b1312-0399-4223-ff5d-576875baa7e8"
   },
   "outputs": [],
   "source": [
    "# all create a plus sign\n",
    "plt.scatter(csv_train[\"t_start_cntr\"],csv_train[\"t_tof\"]) # a lot more 0s\n",
    "plt.title(\"t_tof vs t_start_cntr\")\n",
    "plt.xlabel(\"t_start_cntr\")\n",
    "plt.ylabel(\"t_tof\")\n",
    "plt.show()\n",
    "\n",
    "# plt.hist(csv_train[\"t_start_cntr\"],bins=25) # -60 to ~50\n",
    "# plt.hist(csv_train[\"t_tof\"],bins=25) # ~-120 to ~175\n",
    "# plt.hist(csv_train[\"t_bcal\"],bins=25) # ~-22 to 20\n",
    "# plt.hist(csv_train[\"t_fcal\"],bins=25) # ~-100 to ~75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "IFF2_7npOseE",
    "outputId": "8e8bec48-6610-41d1-8faa-64814bb75af9"
   },
   "outputs": [],
   "source": [
    "plt.hist(csv_train[\"hit1_glayer\"],bins=24)\n",
    "plt.hist(csv_train[\"hit2_glayer\"],bins=24)\n",
    "plt.hist(csv_train[\"hit3_glayer\"],bins=24)\n",
    "plt.hist(csv_train[\"hit4_glayer\"],bins=24)\n",
    "plt.hist(csv_train[\"hit5_glayer\"],bins=24)\n",
    "plt.hist(csv_train[\"hit6_glayer\"],bins=24)\n",
    "plt.hist(csv_train[\"hit10_glayer\"],bins=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdRImNvKZ0Fa"
   },
   "source": [
    "### 2D Scatters of various hit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mxK94YV2FM2"
   },
   "outputs": [],
   "source": [
    "# Oval\n",
    "plt.scatter(csv_train[\"hit1_u\"],csv_train[\"hit1_v\"]) # -3 to 3, even distrib\n",
    "plt.title(\"v vs u\")\n",
    "plt.xlabel(\"u\")\n",
    "plt.ylabel(\"v\")\n",
    "plt.show()\n",
    "\n",
    "# like a flame\n",
    "plt.scatter(csv_train[\"hit1_s\"],csv_train[\"hit1_ds\"]) # -3 to 3, even distrib\n",
    "plt.title(\"ds vs s\")\n",
    "plt.xlabel(\"s\")\n",
    "plt.ylabel(\"ds\")\n",
    "plt.show()\n",
    "\n",
    "# hit1_wire, with single letters, forms an oval\n",
    "plt.scatter(csv_train[\"hit1_wire\"],csv_train[\"hit1_s\"]) # -3 to 3, even distrib\n",
    "plt.title(\"hit1_s vs hit1_wire\")\n",
    "plt.xlabel(\"hit1_wire\")\n",
    "plt.ylabel(\"hit1_s\")\n",
    "plt.show()\n",
    "\n",
    "# go up in steps\n",
    "plt.scatter(csv_train[\"hit1_glayer\"],csv_train[\"hit1_z\"]) # -3 to 3, even distrib\n",
    "plt.title(\"z vs glayer\")\n",
    "plt.xlabel(\"glayer\")\n",
    "plt.ylabel(\"z\")\n",
    "plt.show()\n",
    "\n",
    "# 1:1\n",
    "plt.scatter(csv_train[\"hit1_q\"],csv_train[\"hit1_dE_amp\"]) # -3 to 3, even distrib\n",
    "plt.title(\"dE_amp vs q\")\n",
    "plt.xlabel(\"q\")\n",
    "plt.ylabel(\"dE_amp\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhNGqPbNQ8fX"
   },
   "outputs": [],
   "source": [
    "aax = \n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dM6aySx8xO-"
   },
   "source": [
    "# Depricated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ie8Rhqf765N5"
   },
   "source": [
    "## Non Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpJHFBHy74vp"
   },
   "outputs": [],
   "source": [
    "# --==Not in use?==--\n",
    "# lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate=1e-3,\n",
    "#     decay_steps=10000,\n",
    "#     decay_rate=0.8)\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "nInput = 10\n",
    "\n",
    "# inputs = keras.layers.Input((None,nInput))\n",
    "# print(\"train shape of one batch:\", x_train.shape[1:])\n",
    "\n",
    "# --==Set seed to get identical results==-- begin\n",
    "# from tensorflow.random import set_seed\n",
    "# np.random.seed(1)\n",
    "# set_seed(2)\n",
    "# --==Set seed to get identical results==-- end\n",
    "\n",
    "\n",
    "#--==Set Weights==--\n",
    "# loss_weights = [1/(sd**2)]\n",
    "# loss_weights = np.array(loss_weights)/sum(loss_weights)\n",
    "# model.compile(optimizer=optimizer, loss=\"mse\", loss_weights=loss_weights, metrics=[\"mae\"])\n",
    "\n",
    "inputs = keras.Input((None,nInput),ragged=True)\n",
    "\n",
    "# --==Choose model==--\n",
    "# x = model(inputs)\n",
    "# x = model_timeless(inputs)\n",
    "# x = RNNTime(inputs)\n",
    "x = RNNTimeless(inputs)\n",
    "# x = RNNTimeStateful(inputs)\n",
    "\n",
    "\n",
    "outs = {\n",
    "    \"q_pt\":Dense(1, name=\"q_pt\")(x),\n",
    "    \"phi\":Dense(1, name=\"phi\")(x),\n",
    "    \"tanl\":Dense(1, name=\"tanl\")(x),\n",
    "    \"D\":Dense(1, name=\"D\")(x),\n",
    "    \"z\":Dense(1, name=\"z\")(x)\n",
    "}\n",
    "\n",
    "y_dict = {\n",
    "    \"q_pt\":y_train[:,0],\n",
    "    \"phi\":y_train[:,1],\n",
    "    \"tanl\":y_train[:,2],\n",
    "    \"D\":y_train[:,3],\n",
    "    \"z\":y_train[:,4]\n",
    "}\n",
    "\n",
    "\n",
    "# model = keras.Model(inputs=inputs, outputs=x, name=\"RNNModel\")\n",
    "# model = keras.Model(inputs=inputs, outputs=x, name=\"RNNModel\")\n",
    "model = keras.Model(inputs=inputs, outputs=outs, name=\"RNNModel\")\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hw07sS4jFgJI"
   },
   "outputs": [],
   "source": [
    "es = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.01, patience=50, mode='min', verbose=1, restore_best_weights=True)\n",
    "rag_x = x_train[0]\n",
    "H = model.fit(x=rag_x, y=y_dict, batch_size=64, epochs=100, verbose=1, callbacks=[es])\n",
    "\n",
    "# Overfit\n",
    "# es = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.001, patience=100, mode='min', verbose=1, restore_best_weights=True)\n",
    "# H = model.fit(x=x_train[:10], y=y_train[:10], batch_size=1, epochs=200, verbose=1, shuffle=True, callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMcu1m3k7rHK"
   },
   "source": [
    "## Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUBj5VjBuyL9"
   },
   "source": [
    "### V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R7-v4CJdyQk8",
    "outputId": "940df566-bd20-4356-a2cb-da3bcfaa5fd9"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "#--------------------------------------------\n",
    "# Define custom loss function \n",
    "def customLoss(y_true, y_pred, invcov):\n",
    "  # print(type(y_true))    #<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
    "\n",
    "\n",
    "  # print(y_pred)\n",
    "\n",
    "  y_pred_a = []\n",
    "  for k in y_pred.keys():\n",
    "    y_pred_a.append(np.squeeze(y_pred[k]))\n",
    "\n",
    "  y_pred = np.array(y_pred_a).astype(\"float64\")\n",
    "  y_pred = tf.transpose(y_pred, perm=(1,0))\n",
    "  # print(y_pred.shape)\n",
    "  print(y_pred)\n",
    "\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "  print('y_pred shape: ' + str(y_pred.shape) )  # y_pred dict shape of each is (batch, 1)\n",
    "  print('y_true shape: ' + str(y_true.shape) )  # y_true shape is (batch, 5)\n",
    "  print('invcov shape: ' + str(invcov.shape) )  # invcov shape is (batch, 25)\n",
    "  \n",
    "  y_pred = K.reshape(y_pred, (batch_size, 5,1)) # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = K.reshape(y_true, (batch_size, 5,1)) # y_state shape is now (batch, 5,1)\n",
    "  invcov = K.reshape(invcov, (batch_size, 5,5)) # invcov  shape is now (batch, 5,5)\n",
    "  \n",
    "  # n.b. we must use tf.transpose here an not K.transpose since the latter does not allow perm argument\n",
    "  invcov = tf.transpose(invcov, perm=[0,2,1])     # invcov shape is now (batch, 5,5)\n",
    "  \n",
    "  # Difference between prediction and true state vectors\n",
    "  y_diff = y_pred - y_true\n",
    "\n",
    "  # n.b. use \"batch_dot\" and not \"dot\"!\n",
    "  y_dot = K.batch_dot(invcov, y_diff)           # y_dot shape is (batch,5,1)\n",
    "  y_loss = K.reshape(y_dot, (batch_size, 5))  # y_dot shape is now (batch,5)\n",
    "\n",
    "  y_dict = {\n",
    "      \"q_pt\":y_loss[:,0],\n",
    "      \"phi\":y_loss[:,1],\n",
    "      \"tanl\":y_loss[:,2],\n",
    "      \"D\":y_loss[:,3],\n",
    "      \"z\":y_loss[:,4],\n",
    "  }\n",
    "\n",
    "  # y_dot = K.reshape(y_dot, (batch_size, 1, 5))  # y_dot shape is now (batch,1,5)\n",
    "  # y_loss = K.batch_dot(y_dot, y_diff)           # y_loss shape is (batch,1,1)\n",
    "  # y_loss = K.reshape(y_loss, (batch_size,))     # y_loss shape is now (batch)\n",
    "  return y_dict\n",
    "#--------------------------------------------\n",
    "# Test loss function\n",
    "# x_test = y_train[0]\n",
    "x_test = x_train[2][0:4]\n",
    "y_test = model.predict([x_train[0][0:4],x_train[1][0:4],x_train[2][0:4]])\n",
    "inconv_test = x_train[1][0:4]\n",
    "\n",
    "# for k in y_test.keys():\n",
    "#   y_test[k] = np.squeeze(y_test[k])\n",
    "# print(y_test)\n",
    "\n",
    "# print(y_test)\n",
    "# y_test_a = []\n",
    "# for k in y_test.keys():\n",
    "#   y_test_a.append(y_test[k])\n",
    "# y_test = np.squeeze(np.array(y_test_a))\n",
    "# print(y_test.shape)\n",
    "# print(y_test)\n",
    "\n",
    "# loss = K.eval(customLoss(K.variable([x_test,x_test,x_test]), K.variable([y_test,y_test,y_test]), K.variable([inconv_test,inconv_test,inconv_test])))\n",
    "loss = K.eval(customLoss(x_test, y_test, inconv_test))\n",
    "# print('loss shape: '    + str(loss.shape)    )\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAP0kzRvu2hz"
   },
   "source": [
    "### V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7mcvkxuuweK",
    "outputId": "1197158f-904e-4103-f45a-2f015a71f639"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "#--------------------------------------------\n",
    "# Define custom loss function \n",
    "def customLoss(q_pt_true, phi_true, tanl_true, D_true, z_true, q_pt_pred, phi_pred, tanl_pred, D_pred, z_pred, invcov):\n",
    "\n",
    "\n",
    "  y_pred = [q_pt_pred, phi_pred, tanl_pred, D_pred, z_pred]\n",
    "  # y_pred = np.array(y_pred).astype(\"float64\")\n",
    "  y_pred = tf.transpose(y_pred, perm=(1,0))\n",
    "  y_pred = tf.cast(y_pred, \"float64\")\n",
    "\n",
    "  y_true = [q_pt_true, phi_true, tanl_true, D_true, z_true]\n",
    "  # y_true = np.array(y_true).astype(\"float64\")\n",
    "  y_true = tf.transpose(y_true, perm=(1,0))\n",
    "  y_true = tf.cast(y_true, \"float64\")\n",
    "\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "  print('y_pred shape: ' + str(y_pred.shape) )  # y_pred dict shape of each is (batch, 1)\n",
    "  print('y_true shape: ' + str(y_true.shape) )  # y_true shape is (batch, 5)\n",
    "  print('invcov shape: ' + str(invcov.shape) )  # invcov shape is (batch, 25)\n",
    "  \n",
    "  y_pred = K.reshape(y_pred, (batch_size, 5,1)) # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = K.reshape(y_true, (batch_size, 5,1)) # y_state shape is now (batch, 5,1)\n",
    "  invcov = K.reshape(invcov, (batch_size, 5,5)) # invcov  shape is now (batch, 5,5)\n",
    "  \n",
    "  # n.b. we must use tf.transpose here an not K.transpose since the latter does not allow perm argument\n",
    "  invcov = tf.transpose(invcov, perm=[0,2,1])     # invcov shape is now (batch, 5,5)\n",
    "  \n",
    "  # Difference between prediction and true state vectors\n",
    "  y_diff = y_pred - y_true\n",
    "\n",
    "  # n.b. use \"batch_dot\" and not \"dot\"!\n",
    "  y_dot = K.batch_dot(invcov, y_diff)           # y_dot shape is (batch,5,1)\n",
    "  y_loss = K.reshape(y_dot, (batch_size, 5))  # y_dot shape is now (batch,5)\n",
    "\n",
    "  y_dict = {\n",
    "      \"q_pt\":y_loss[:,0],\n",
    "      \"phi\":y_loss[:,1],\n",
    "      \"tanl\":y_loss[:,2],\n",
    "      \"D\":y_loss[:,3],\n",
    "      \"z\":y_loss[:,4],\n",
    "  }\n",
    "\n",
    "  # y_dot = K.reshape(y_dot, (batch_size, 1, 5))  # y_dot shape is now (batch,1,5)\n",
    "  # y_loss = K.batch_dot(y_dot, y_diff)           # y_loss shape is (batch,1,1)\n",
    "  # y_loss = K.reshape(y_loss, (batch_size,))     # y_loss shape is now (batch)\n",
    "  return y_dict\n",
    "#--------------------------------------------\n",
    "# Test loss function\n",
    "# x_test = y_train[0]\n",
    "x_test = x_train[2][0:4]\n",
    "y_test = model.predict([x_train[0][0:4],x_train[1][0:4],x_train[2][0:4]])\n",
    "inconv_test = x_train[1][0:4]\n",
    "\n",
    "y_test_a = []\n",
    "for k in y_test.keys():\n",
    "  y_test_a.append(y_test[k])\n",
    "y_test = np.squeeze(np.array(y_test_a))\n",
    "y_test = np.swapaxes(y_test, 0, 1)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "# loss = K.eval(customLoss(K.variable([x_test,x_test,x_test]), K.variable([y_test,y_test,y_test]), K.variable([inconv_test,inconv_test,inconv_test])))\n",
    "loss = K.eval(customLoss(x_test[:,0],x_test[:,1],x_test[:,2],x_test[:,3],x_test[:,4], y_test[:,0], y_test[:,1],y_test[:,2],y_test[:,3],y_test[:,4], inconv_test))\n",
    "# print('loss shape: '    + str(loss.shape)    )\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kuUpxAuz1sz"
   },
   "source": [
    "### V4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRUvQVjHAD2U"
   },
   "source": [
    "So, we have the prediction and true vector\n",
    "\n",
    "$$\n",
    "y_{pred}=\n",
    "\\begin{bmatrix}\n",
    "q\\_pt \\\\ phi \\\\ tanl \\\\ D \\\\ z\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We have the inverse covariance matrix, we'll label it $C^{-1}$:\n",
    "\n",
    "and $y_p = y_{predict}$\n",
    "\n",
    "$$\n",
    "C^{-1} = \n",
    "\\begin{bmatrix}\n",
    "qq & qp & qt & qd & qz \\\\\n",
    "qp & pp & pt & pd & pz \\\\\n",
    "qt & pt & tt & td & tz \\\\\n",
    "qd & pd & td & dd & dz \\\\\n",
    "qz & pz & tz & dz & zz \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Thus, the formula before was:\n",
    "\n",
    "$$\n",
    "loss = C^{-1} \\cdot \\vec{y_p}  \\cdot \\vec{y_p}\n",
    "$$\n",
    "\n",
    "$$\n",
    "  y_{dot} =\n",
    "  \\begin{bmatrix}\n",
    "    qq & qp & qt & qd & qz \\\\\n",
    "    qp & pp & pt & pd & pz \\\\\n",
    "    qt & pt & tt & td & tz \\\\\n",
    "    qd & pd & td & dd & dz \\\\\n",
    "    qz & pz & tz & dz & zz \n",
    "  \\end{bmatrix} \n",
    "  \\cdot\n",
    "  \\begin{bmatrix}\n",
    "    q\\_pt \\\\ phi \\\\ tanl \\\\ D \\\\ z\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "  y_{dot} = \n",
    "  \\begin{bmatrix}\n",
    "    qq*q\\_pt + qp*phi + qt*tanl + qd*D + qz*z \\\\\n",
    "    qp*q\\_pt + pp*phi + pt*tanl + pd*D + pz*z \\\\\n",
    "    qt*q\\_pt + pt*phi + tt*tanl + td*D + tz*z \\\\\n",
    "    qd*q\\_pt + pd*phi + td*tanl + dd*D + dz*z \\\\\n",
    "    qz*q\\_pt + dz*phi + tz*tanl + pz*D + zz*z\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Now, working on the output split for each variable, im looking for ways to seperate the variables after that operation.\n",
    "So maybe just sum the q_pt column of the matrix and multiply by q_pt?\n",
    "\n",
    "Maybe this would work? :\n",
    "\n",
    "$$\n",
    "loss_{q\\_pt} =  y_p^{q\\_pt} * \\sum_{i=0}^{4} C^{-1}_{qi}\n",
    "$$\n",
    "\n",
    "Where $C^{-1}_q$ is one row or column of the matrix of that variable\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8shIh2axz1g_",
    "outputId": "bbc9620f-ee91-4ed5-a02c-8e29e698e46b"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "#--------------------------------------------\n",
    "# Define custom loss function \n",
    "def customLoss(y_true, y_pred, invcov):\n",
    "  # print(type(y_true))    #<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
    "\n",
    "  # print(y_pred)\n",
    "\n",
    "  # Theoretically:\n",
    "  # K.dot(invcov[0,:] * q_pt,q_pt)    # ?\n",
    "  \n",
    "\n",
    "\n",
    "  y_pred_a = []\n",
    "  for k in y_pred.keys():\n",
    "    y_pred_a.append(np.squeeze(y_pred[k]))\n",
    "\n",
    "  y_pred = np.array(y_pred_a).astype(\"float64\")\n",
    "  y_pred = tf.transpose(y_pred, perm=(1,0))\n",
    "  # print(y_pred.shape)\n",
    "  print(y_pred)\n",
    "\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "  print('y_pred shape: ' + str(y_pred.shape) )  # y_pred dict shape of each is (batch, 1)\n",
    "  print('y_true shape: ' + str(y_true.shape) )  # y_true shape is (batch, 5)\n",
    "  print('invcov shape: ' + str(invcov.shape) )  # invcov shape is (batch, 25)\n",
    "  \n",
    "  y_pred = K.reshape(y_pred, (batch_size, 5,1)) # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = K.reshape(y_true, (batch_size, 5,1)) # y_state shape is now (batch, 5,1)\n",
    "  invcov = K.reshape(invcov, (batch_size, 5,5)) # invcov  shape is now (batch, 5,5)\n",
    "  \n",
    "  # n.b. we must use tf.transpose here an not K.transpose since the latter does not allow perm argument\n",
    "  invcov = tf.transpose(invcov, perm=[0,2,1])     # invcov shape is now (batch, 5,5)\n",
    "  \n",
    "  # Difference between prediction and true state vectors\n",
    "  y_diff = y_pred - y_true\n",
    "\n",
    "  # n.b. use \"batch_dot\" and not \"dot\"!\n",
    "  y_dot = K.batch_dot(invcov, y_diff)           # y_dot shape is (batch,5,1)\n",
    "  y_loss = K.reshape(y_dot, (batch_size, 5))  # y_dot shape is now (batch,5)\n",
    "\n",
    "  y_dict = {\n",
    "      \"q_pt\":y_loss[:,0],\n",
    "      \"phi\":y_loss[:,1],\n",
    "      \"tanl\":y_loss[:,2],\n",
    "      \"D\":y_loss[:,3],\n",
    "      \"z\":y_loss[:,4],\n",
    "  }\n",
    "\n",
    "  # y_dot = K.reshape(y_dot, (batch_size, 1, 5))  # y_dot shape is now (batch,1,5)\n",
    "  # y_loss = K.batch_dot(y_dot, y_diff)           # y_loss shape is (batch,1,1)\n",
    "  # y_loss = K.reshape(y_loss, (batch_size,))     # y_loss shape is now (batch)\n",
    "  return y_dict\n",
    "#--------------------------------------------\n",
    "# Test loss function\n",
    "# x_test = y_train[0]\n",
    "x_test = x_train[2][0:4]\n",
    "y_test = model.predict([x_train[0][0:4],x_train[1][0:4],x_train[2][0:4]])\n",
    "inconv_test = x_train[1][0:4]\n",
    "\n",
    "# for k in y_test.keys():\n",
    "#   y_test[k] = np.squeeze(y_test[k])\n",
    "# print(y_test)\n",
    "\n",
    "# print(y_test)\n",
    "# y_test_a = []\n",
    "# for k in y_test.keys():\n",
    "#   y_test_a.append(y_test[k])\n",
    "# y_test = np.squeeze(np.array(y_test_a))\n",
    "# print(y_test.shape)\n",
    "# print(y_test)\n",
    "\n",
    "# loss = K.eval(customLoss(K.variable([x_test,x_test,x_test]), K.variable([y_test,y_test,y_test]), K.variable([inconv_test,inconv_test,inconv_test])))\n",
    "loss = K.eval(customLoss(x_test, y_test, inconv_test))\n",
    "# print('loss shape: '    + str(loss.shape)    )\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7htvZHbENgC7"
   },
   "source": [
    "### V5 unedited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hK6jqei9Nf5W"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "#--------------------------------------------\n",
    "# Define custom loss function \n",
    "def customLoss(y_true, y_pred, invcov):\n",
    "  # print(type(y_true))    #<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
    "\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "  print('y_pred shape: ' + str(y_pred.shape) )  # y_pred shape is (batch, 5)\n",
    "  print('y_true shape: ' + str(y_true.shape) )  # y_true shape is (batch, 5)\n",
    "  print('invcov shape: ' + str(invcov.shape) )  # invcov shape is (batch, 25)\n",
    "  \n",
    "  y_pred = K.reshape(y_pred, (batch_size, 5,1)) # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = K.reshape(y_true, (batch_size, 5,1)) # y_state shape is now (batch, 5,1)\n",
    "  invcov = K.reshape(invcov, (batch_size, 5,5)) # invcov  shape is now (batch, 5,5)\n",
    "  \n",
    "  # n.b. we must use tf.transpose here an not K.transpose since the latter does not allow perm argument\n",
    "  invcov = tf.transpose(invcov, perm=[0,2,1])     # invcov shape is now (batch, 5,5)\n",
    "  \n",
    "  # Difference between prediction and true state vectors\n",
    "  y_diff = y_pred - y_true\n",
    "\n",
    "  # n.b. use \"batch_dot\" and not \"dot\"!\n",
    "  y_dot = K.batch_dot(invcov, y_diff)           # y_dot shape is (batch,5,1)\n",
    "  y_dot = K.reshape(y_dot, (batch_size, 1, 5))  # y_dot shape is now (batch,1,5)\n",
    "  y_loss = K.batch_dot(y_dot, y_diff)           # y_loss shape is (batch,1,1)\n",
    "  y_loss = K.reshape(y_loss, (batch_size,))     # y_loss shape is now (batch)\n",
    "  # y_dict = {\n",
    "  #     \"q_pt\":y_diff[0]*y_diff[0],\n",
    "  #     \"phi\":y_diff[0]*y_diff[0]\n",
    "  # }\n",
    "  # y_diff[0] / invcov[0][0]\n",
    "  return y_loss\n",
    "\n",
    "#--------------------------------------------\n",
    "# Test loss function\n",
    "# x_test = x_train[2][0]\n",
    "# y_test = model.predict([x_train[0][0:1],x_train[1][0:1],x_train[2][0:1]])\n",
    "# y_test = np.squeeze(y_test)\n",
    "# inconv_test = x_train[1][0]\n",
    "\n",
    "# loss = K.eval(customLoss(K.variable([x_test,x_test,x_test]), K.variable([y_test,y_test,y_test]), K.variable([inconv_test,inconv_test,inconv_test])))\n",
    "# # print('loss shape: '    + str(loss.shape)    )\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4XBVGilP4tc"
   },
   "source": [
    "### V6 New Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsGirPS0P4jb"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "def customLoss(m_invcov):\n",
    "  def customLoss_fn(y_true, y_pred):\n",
    "    batch_size = tf.shape(y_pred)[0]\n",
    "\n",
    "    y_pred = tf.cast(K.reshape(y_pred, (batch_size, 5,1)),\"float64\") # y_pred  shape is now (batch, 5,1)\n",
    "    y_true = tf.cast(K.reshape(y_true, (batch_size, 5,1)),\"float64\") # y_state shape is now (batch, 5,1)\n",
    "    invcov = tf.cast(K.reshape(m_invcov, (batch_size, 5,5)),\"float64\") # invcov  shape is now (batch, 5,5)\n",
    "    \n",
    "    # n.b. we must use tf.transpose here an not K.transpose since the latter does not allow perm argument\n",
    "    invcov = tf.transpose(invcov, perm=[0,2,1])     # invcov shape is now (batch, 5,5)\n",
    "    \n",
    "    # Difference between prediction and true state vectors\n",
    "    y_diff = y_pred - y_true\n",
    "\n",
    "    # n.b. use \"batch_dot\" and not \"dot\"!\n",
    "    y_dot = K.batch_dot(invcov, y_diff)           # y_dot shape is (batch,5,1)\n",
    "    y_dot = K.reshape(y_dot, (batch_size, 1, 5))  # y_dot shape is now (batch,1,5)\n",
    "    y_loss = K.batch_dot(y_dot, y_diff)           # y_loss shape is (batch,1,1)\n",
    "    y_loss = K.reshape(y_loss, (batch_size,))     # y_loss shape is now (batch)\n",
    "    return y_loss\n",
    "  return customLoss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkuUUNJWjM_i"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "def customLoss(y_true, y_pred, invcov):\n",
    "  batch_size = tf.shape(y_pred)[0]\n",
    "\n",
    "  print('y_pred shape: ' + str(y_pred.shape) )  # y_pred shape is (batch, 5)\n",
    "  print('y_true shape: ' + str(y_true.shape) )  # y_true shape is (batch, 5)\n",
    "  print('invcov shape: ' + str(invcov.shape) )  # invcov shape is (batch, 25)\n",
    "\n",
    "  y_pred = tf.cast(K.reshape(y_pred, (batch_size, 5,1)),\"float64\") # y_pred  shape is now (batch, 5,1)\n",
    "  y_true = tf.cast(K.reshape(y_true, (batch_size, 5,1)),\"float64\") # y_state shape is now (batch, 5,1)\n",
    "  invcov = tf.cast(K.reshape(invcov, (batch_size, 5,5)),\"float64\") # invcov  shape is now (batch, 5,5)\n",
    "  \n",
    "  # n.b. we must use tf.transpose here an not K.transpose since the latter does not allow perm argument\n",
    "  invcov = tf.transpose(invcov, perm=[0,2,1])     # invcov shape is now (batch, 5,5)\n",
    "  \n",
    "  # Difference between prediction and true state vectors\n",
    "  y_diff = y_pred - y_true\n",
    "\n",
    "  # n.b. use \"batch_dot\" and not \"dot\"!\n",
    "  y_dot = K.batch_dot(invcov, y_diff)           # y_dot shape is (batch,5,1)\n",
    "  y_dot = K.reshape(y_dot, (batch_size, 1, 5))  # y_dot shape is now (batch,1,5)\n",
    "  y_loss = K.batch_dot(y_dot, y_diff)           # y_loss shape is (batch,1,1)\n",
    "  y_loss = K.reshape(y_loss, (batch_size,))     # y_loss shape is now (batch)\n",
    "  return y_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqNp5hG98yPr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "DBknRRcl6LMm"
   ],
   "name": "CopyOfRNN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
